{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e966b60",
   "metadata": {},
   "source": [
    "until now, all neural networks introduced have been implemented using the 'Sequential' model; the sequential model makes the assumption that the network has exactly one input and exactly one output, and that it consists of a linear stack of layers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8053a45",
   "metadata": {},
   "source": [
    "![A Sequential Model: A Linear Stack of Layer](./seq_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b424ec",
   "metadata": {},
   "source": [
    "![A Multi-input Model](./mul_input.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97797c6",
   "metadata": {},
   "source": [
    "![A Multi-output (or Multihead) Model](./mul_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c4129",
   "metadata": {},
   "source": [
    "![Graph-Like Model](./graph_like_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1fecb4",
   "metadata": {},
   "source": [
    "### Introduction to the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd0f039",
   "metadata": {},
   "source": [
    "in the functional API, you directly manipulate tensors, and use layers as fucntions that take tensors and renturn tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4dd0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, layers\n",
    "\n",
    "input_tensor = Input(shape=(32,)) #  a tensor\n",
    "\n",
    "dense = layers.Dense(32, activation='relu') # a layer function\n",
    "\n",
    "output_tensor = dense(input_tensor) # a layer may be called on a tensor, and it returns a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daa45d2",
   "metadata": {},
   "source": [
    "a minimal example that shows side by side a simple Sequential model and its equivalent in the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47af36df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 64)]              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_13 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "# sequential model\n",
    "seq_model = Sequential()\n",
    "seq_model.add(layers.Dense(32, activation='relu', input_shape=(64,)))\n",
    "seq_model.add(layers.Dense(32, activation='relu'))\n",
    "seq_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# its functional equivalent\n",
    "input_tensor = Input(shape=(64,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "model = Model(input_tensor, output_tensor) # the model class turns an input tensor and outptu tensor into a model\n",
    "\n",
    "model.summary()\n",
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b29ca20",
   "metadata": {},
   "source": [
    "the API is the same as that of Sequential when it comes to compiling, training, or evaluating such an instance of model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train = np.random.random((1000, 64))\n",
    "y_train = np.random.random((1000, 10))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128)\n",
    "score = model.evaluate(x_train, y_train)\n",
    "\n",
    "seq_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "seq_model.fit(x_train, y_train, epochs=10, batch_size=128)\n",
    "seq_score = seq_model.evaluate(x_train, y_train)\n",
    "\n",
    "print(score, seq_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbe79dc",
   "metadata": {},
   "source": [
    "### Multi-Input Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e41a6d",
   "metadata": {},
   "source": [
    "![A Question-Answering Model](./qa_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69efe5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functional API implementation of a two-input question-answering model\n",
    "from keras.models import Model\n",
    "from keras import layers \n",
    "from keras import Input\n",
    "\n",
    "text_vocabulary_size = 10000\n",
    "questioni_vocabulary_size = 10000\n",
    "answer_vocabulary_size = 500\n",
    "\n",
    "text_input = Input(shape=(None,), dtype='int32', name='text') # a variable-length sequence of integers\n",
    "\n",
    "# embedding the inputs into a sequence of vector of size 64\n",
    "embedded_text = layers.Embedding(64, text_vocabulary_size)(text_input)\n",
    "\n",
    "# encodes the vectors in a single vector via an LSTM\n",
    "encoded_text = layers.LSTM(32)(embedded_text)\n",
    "\n",
    "# some process (with different layer instances) for the question\n",
    "question_input = Input(shape=(None,),\n",
    "                       dtype='int32', \n",
    "                       name='question')\n",
    "\n",
    "embedded_question = layers.Embedding(32, questioni_vocabulary_size)(question_input)\n",
    "encoded_question = layers.LSTM(16)(embedded_question)\n",
    "\n",
    "# adds a softmax classifier on top\n",
    "concatenated = layers.concatenate([encoded_text, encoded_question], axis=-1)\n",
    "\n",
    "# add a softmax classifier on top\n",
    "answer = layers.Dense(answer_vocabulary_size, activation='softmax')(concatenated)\n",
    "\n",
    "# at model instantiation, specify the two inputs and the output\n",
    "model = Model([text_input, question_input], answer)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fccdc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 4s 72ms/step - loss: 0.0000e+00 - acc: 0.0610\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 51ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28fb11282b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feeding data to a multi-input model\n",
    "import numpy as np\n",
    "\n",
    "num_samples = 1000\n",
    "max_length =100\n",
    "\n",
    "# generate dummy numpy data\n",
    "text = np.random.randint(1, text_vocabulary_size, size=(num_samples, max_length))\n",
    "\n",
    "question = np.random.randint(1, questioni_vocabulary_size, size=(num_samples, max_length))\n",
    "answers = np.random.randint(0, 1, size=(num_samples, answer_vocabulary_size)) # answers are one-hot encoded, not integers\n",
    "\n",
    "model.fit([text, question], answers, epochs=10, batch_size=128) # fitting using a list of inputs\n",
    "model.fit({'text':text, 'question':question}, answers, epochs=10, batch_size=128) # fitting using a dictionary of inputs (only if inputs are named)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4828fd15",
   "metadata": {},
   "source": [
    "![A Social Media Model with Three Heads](./sm_model_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a8958a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functional API implementation of a three-output model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "\n",
    "vocabulary_size = 50000\n",
    "num_income_groups = 10\n",
    "\n",
    "posts_input = Input(shape=(None,), dtype='int32', name='post')\n",
    "embedded_posts = layers.Embedding(256, vocabulary_size)(posts_input)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(embedded_posts)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "age_prediction = layers.Dense(1, name='age')(x) # note that the outptu layers are given names\n",
    "income_prediction = layers.Dense(num_income_groups, activation='softmax', name='income')(x)\n",
    "gender_prediction = layers.Dense(1, activation='sigmoid', name='gender')(x)\n",
    "model = Model(posts_input, [age_prediction, income_prediction, gender_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c4914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilation options of a multi-output: multiple losses\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss=['mae', 'categorical_crossentropy', 'binary_crossentropy'])\n",
    "\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss={'age': 'mse', # equivalent (possible only if you give names to the output layers)\n",
    "                    'income': 'categorical_crossentropy',\n",
    "                    'gender': 'bianry_crossentropy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d54ee7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilation options of a multi-output model: loss weighting\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'],\n",
    "              loss_weights=[0.25, 1., 10.])\n",
    "\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss={'age': 'mse', # equivalent (possible only if you give names to the output layers)\n",
    "                    'income': 'categorical_crossentropy',\n",
    "                    'gender': 'bianry_crossentropy'},\n",
    "              loss_weights={'age': 0.25,\n",
    "                            'income':1.,\n",
    "                            'gender':10.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925311b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# feeding data to a multi-output model\n",
    "model.fit(posts, [age_targets, income_targets, gender_targets], # assume targets to be numpy array\n",
    "          epochs=10, batch_size=64)\n",
    "\n",
    "model.fit(posts, {'age': age_targets, # equivalent (possible only if you give names to the output layers)\n",
    "                  'income': income_targets,\n",
    "                  'gender': gender_targets},\n",
    "          epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac7c5d",
   "metadata": {},
   "source": [
    "### Directed Acyclic Graphs of Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15a7c8e",
   "metadata": {},
   "source": [
    "- with the functional API, you can also implement networks with a complex internal topology; neural networks in keras are allowed to be arbitraty directed acyclic graphs of layers\n",
    "- the qualifier acyclic is important: these graphs can't have cycles, it's impossible for a tensor x to become the input of one of the layers that generated x\n",
    "- the only processing loops that are allowed (recurrent connections) are those internal to recurrent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7087b",
   "metadata": {},
   "source": [
    "two notable common neural-network components: Inception Modules and Residual Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd448c",
   "metadata": {},
   "source": [
    "##### Inception Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8604c3d",
   "metadata": {},
   "source": [
    "a popular type of network architecture for convolutional neural networks; it consists of a stack of modules that hemselves look like small independent networks, split into several parallel branches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a366f",
   "metadata": {},
   "source": [
    "![An Inception Module](./inception_module.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68027cca",
   "metadata": {},
   "source": [
    "##### The Purpose of 1 x 1 Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589e6d2",
   "metadata": {},
   "source": [
    "convolutions extract spatial patches around evey tile in an input tensor and apply the same transformation to each patch; an edge case is when the patches extracted consist of a single tile, the convolution operation when becomes equivalent to running each tile vector through a Dense layer: it will compute features that mix together information from the channels of the input tensor, but it won't mix information across space (because it's looking at one tile at a time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b16ef0",
   "metadata": {},
   "source": [
    "such 1 x 1 convolutions (also called poinrtwise concolutions) are featured in inception modules, where they contribute to factoring out channel-wise feature learning and space-wise feature learning -- a reasonable thing to do if you assume that each channel is highly autocorrelated across space, but different channels may not be highly correlated with each other "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c427b2d",
   "metadata": {},
   "source": [
    "example assumes the existence of a 4D input tensor x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "# every branch has the same stride value (2), which is necessary to keep all branch outputs the same size so you can concatenate them\n",
    "branch_a = layers.Conv2D(128, 1,\n",
    "                         activation='relu', strides=2)(x)\n",
    "\n",
    "# the striding occurs in the spatial convolution layer in branch_b \n",
    "branch_b = layers.Conv2D(128, 1, activation='relu')(x)\n",
    "branch_b = layers.Conv2D(128, 3, activation='relu', strides=2)(branch_b)\n",
    "\n",
    "# the striding occurs in the average pooling layer\n",
    "branch_c = layers.AveragePooling2D(3, strides=2)(x)\n",
    "branch_c = layers.Conv2D(128, 3, activation='relu')(branch_c)\n",
    "\n",
    "branch_d = layers.Conv2D(128, 1, activation='relu')(x)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu')(branch_d)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu', strides=2)(branch_d)\n",
    "\n",
    "# concatenates teh branch outputs to obtain teh module output\n",
    "output = layers.concatenate([branch_a, branch_b, branch_c, branch_d], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a38a6",
   "metadata": {},
   "source": [
    "- full Inception V3 architecture is availabel in keras as 'keras.applications.inception_v3.InveptionV3', including weights pretrained on the ImageNet dataset\n",
    "- Xception (extreme inception), is a convnet architecture loosely inspired by inception, it takes the idea of separating the learning of channel-wise features to its logical extreme, and replaces inception modules with depth-wise separable convolutions consisting of a depthwise convolution (a spatial concolution where every input channel is handled separately) followed by a pointwise convolution (a 1 x 1 convolution) -- effectively, an extreme form of an Inception module, where spatial features and channel-wise features are fully separated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8401ab2",
   "metadata": {},
   "source": [
    "##### Residual Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d9247b",
   "metadata": {},
   "source": [
    "residual connections are a common graph-like network component tackle two common problems taht plague any large-scale deep-learning model: vanishing gradients and representational bottelnecks; in general, adding residual connections to any model that has more than 10 layers is likely to be beneficial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83256902",
   "metadata": {},
   "source": [
    "a residual connection consist of making the outptu of an earlier layer available as input to a late layer, effectively creating a shortcut in a sequential network; rather than being concatenated to the later activation, the earlier output is summed with the layer activation, which assumes that both activations are the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c910f4fe",
   "metadata": {},
   "source": [
    "example implement a residual connection in keras when the feature-map sizes are the same, using identity residual connections, assuming the existence of a 4D input tensor of x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "x = ...\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x) # applies a transformation to x\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "\n",
    "y = layers.add([y, x]) # adds the original x back to the output features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e93351c",
   "metadata": {},
   "source": [
    "example implement a residual connection when the feature-map sizes differ, using a linear residual connection, assuming the existence of a 4D input tensor x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "x = ...\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.MaxPooling2D(2, strides=2)(y)\n",
    "\n",
    "residual = layers.Conv2D(128, 1, strides=2, padding='same')(x) # use a 1 x 1 convolution to linearly downsample the original x tensor to the same shape as y\n",
    "\n",
    "y = layers.add([y, residual]) # adds the residual tensor back to tht output features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4431a3d",
   "metadata": {},
   "source": [
    "##### Representation Bottlenecks in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ccf47",
   "metadata": {},
   "source": [
    "in a sequential model, each successive representation layer is built on top of the previous one , which means it only has access to information contained in the activation of the previous layer; if one layer is too small, then the model will be constrained by how much informatin can be crammed into the activations of the layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7631844",
   "metadata": {},
   "source": [
    "concept with a signal-processing analogy: if you have an audio processing pipeline that consists of a series of operations, then if one operation crops your signal to a low-frequency range, the operations downstrea will never be able to revocer the dropped frequencies; any loss of information is permanent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47726890",
   "metadata": {},
   "source": [
    "residual connections, by reinjecting earlier information downsteram, partially solve this for deep-lerannig models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35afdf8b",
   "metadata": {},
   "source": [
    "##### Vanishing Gradient in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014a83a1",
   "metadata": {},
   "source": [
    "backpropagation, the master algorithm used to train deep neural networks, works by propagating a feedback signal from the output loss down to earlier layers; if this feedback signal has to be propagated through a depp stack of layers, the signal may become tenuous or even be lost entriely, rendering the network untrainable; this issue is known as vanishing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6e82ee",
   "metadata": {},
   "source": [
    "this problem occurs both with deep networks and with recurrent networks over very long sequences -- in both cases, a feeback signal must be propageted through a long sereis of operations; residual connection work in a similar way as LSTM in feedward deep networks, but simpler: introduce a purely linear information carry track parallel to the main layer stack, thus helping to propagete gradients through arbitrarily deep stacks of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe46bd",
   "metadata": {},
   "source": [
    "### Layer Weight Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b050b46",
   "metadata": {},
   "source": [
    "when call a layer instance twice, instead of instantiating a new layer for each call, reuse the same weights with every call; this allows to build models that have shared branches -- several braches that all share the same knoeledge and perform the same operations; that is, they share the same representations and learn these representations simultaneously for different sets of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf04e5f",
   "metadata": {},
   "source": [
    "example implement a LSTM model using layer sharing (layer reuse) in the keras functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import Input \n",
    "from keras.models import Model\n",
    "\n",
    "lstm = layers.LSTM(32) # instantiates a single LSTM layer once\n",
    "\n",
    "left_input = Input(shape=(None, 128)) # left branch of the moedl\n",
    "left_output = lstm(left_input)\n",
    "\n",
    "right_input = Input(shape=(None, 128)) # right branch of the model: when call an existing layer instance, reuse its weights\n",
    "right_output = lstm(right_input)\n",
    "\n",
    "merged = layers.concatenate([left_output, right_output], axis=-1) # builds the classifier on top\n",
    "predictions = layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model([left_input, right_input], predictions) # instantiating and training the model; the weights of the LSTM layer are updated based on both input\n",
    "model.fit([left_data, right_data], targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee11b8d1",
   "metadata": {},
   "source": [
    "naturally, a layer instance may be used more than once -- it can be called arbitrarily many times, reusing the same set of weights every time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8436b",
   "metadata": {},
   "source": [
    "### Models as Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e76c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x)\n",
    "y1, y2 = model([x1, x2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac4e0a",
   "metadata": {},
   "source": [
    "example implement a Siamese vision model (shared convolutional base) in keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7331f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import applications\n",
    "from keras import Input\n",
    "\n",
    "xception_base = applications.Xception(weights=None, include_top=False) # the base image-processing model is the Xception network (convolutional base only)\n",
    "\n",
    "# the inputs are 250 x 250 RGB images\n",
    "left_input = Input(shape=(250, 250, 3))\n",
    "right_input = Input(shape=(250, 250, 3))\n",
    "\n",
    "# call the same vision model twice\n",
    "left_feature = xception_base(left_input)\n",
    "right_input = xception_base(right_input)\n",
    "\n",
    "merged_features = layers.concatenate([left_feature, right_input], axis=-1)  # the merged features cantain information from the right visual feed and the left visual feed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e2d37",
   "metadata": {},
   "source": [
    "### Wrapping Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e423cec",
   "metadata": {},
   "source": [
    "introduction to the keras functional API -- an essential tool for building advanced deep neural architecture:\n",
    "- to step out the sequential API whenever need anything more than a linear stack of layers\n",
    "- build keras models with several inputs, several outputs and complex internal network topology, using keras functional API\n",
    "- reuse the weight of layers or model accross different processing branches, by calling the same layer or model instance several times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
