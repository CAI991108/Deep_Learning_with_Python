{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df2863d1",
   "metadata": {},
   "source": [
    "### Using Callbacks to Act on a Model During Traning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857445f0",
   "metadata": {},
   "source": [
    "when training a model, you can't tell how many epochs will be needed to get an optimal validation loss; using the first run to figure out the poper number of epochs to tran for can be wasteful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802b6eb",
   "metadata": {},
   "source": [
    "a much better way to handle this is to stop training when measure that the validation loss in no longer improving; this can be achieved using a keras callback\n",
    "- a callback is an object that is passed to the model in the call to 'fit' and that is called by the model at various points during training\n",
    "- it has access to all the availabel data about teh state of the model and its performance, and it can take action: interpt training, save a model, load a different weight set, or otherwise alter the state of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c26f7",
   "metadata": {},
   "source": [
    "some examples of ways use callbacks:\n",
    "- model checkpointing -- saving the current weights of the model at different points during training\n",
    "- early stopping -- interupting training when the validation loss is no longer imporving (and saving the best model obtained during training)\n",
    "- dynamically adjusting the value of certain parameters during training -- such as the learning rate of the optimizer\n",
    "- logging training and validation metrics during training, or visualizing the representations learned by the model as they're uploaded -- the keras progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5c5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some built-in keras.callbacks module:\n",
    "import keras.callbacks\n",
    "\n",
    "keras.callbacks.ModelCheckpoint\n",
    "keras.callbacks.EarlyStopping\n",
    "keras.callbacks.LearningRateScheduler\n",
    "keras.callbacks.ReduceLROnPlateau\n",
    "keras.callbacks.CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a7c0c",
   "metadata": {},
   "source": [
    "##### The ModelCheckpoint and EarlyStopping Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4d9d7",
   "metadata": {},
   "source": [
    "EarlyStopping callback can interrupt training once a target metric being monitored has stopped improving for a fixed number of epochs; this callback is typically used in combination with ModelCheckpoint, which letscontinually save the model during training, save only the current best model so far (the version of the model that achieved the best performance at the end of an epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4cf84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# callbacks are passed to the model via the callbacks argument in fit, which takes a list of callbacks (can pass any number of callbacks)\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping( # interrupts training when improvement stops\n",
    "        monitor='acc', # monitors the model's validation accuracy\n",
    "        patience=1, # interrrupts training when accuracy has stopped improving for more than one epoch (two epochs)\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint( # save the current weights after every epoch \n",
    "        filepath='my_model.h5', # path to the destination model file\n",
    "        monitor='val_loss', # won't overwrite the model file unless val_loss has improved\n",
    "        save_best_only=True, # keep the best model seen during training\n",
    "    )\n",
    "]\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc']) # monitor accuracy (part of the model's metrics)\n",
    "\n",
    "model.fit(x, y, # callback will monitor validation loss and validation accuracy; pass validation_data to the call to fit\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51662110",
   "metadata": {},
   "source": [
    "##### The ReduceLROnPlateau Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7b4de",
   "metadata": {},
   "source": [
    "this callback can reduce the learning rate when the validaiton loss has stopped improving; reducing or increasing the learning rate in case of a loss plateau is an effective strategy to get out of local minima during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2029a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',  # monitors the model's validation loss\n",
    "        factor=0.1, # devides teh learning rate by 10 when triggered\n",
    "        patience=10, # the callback is triggered after the validtion loss has stopped improving for 10 epochs\n",
    "    )\n",
    "]\n",
    "\n",
    "model.fit(x, y, # the callbackwill monitor the validation loss; pass validation_data to the call to fit\n",
    "          epoch=10,\n",
    "          batch_size=32,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f455a0",
   "metadata": {},
   "source": [
    "##### Writting Own Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80387d57",
   "metadata": {},
   "source": [
    "callbacks are implemented by subclassing the class keras.callbacks.Callback; which can implement any number of the following transparently named methods and call at various points during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa94e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_epoch_begin\n",
    "on_epoch_end\n",
    "\n",
    "on_batch_begin\n",
    "on_batch_end\n",
    "\n",
    "on_train_begin\n",
    "on_train_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e4030b",
   "metadata": {},
   "source": [
    "these methods all are called with a logs argument, which is a dictionary contraining information about the previous batch, epoch, or training run: training and validation metrics; additionally, the callback has access to the following attributes:\n",
    "- self.model -- the model instance from which the callback is being called\n",
    "- self.validation_data -- the value of what was passed to fir as validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cebfd",
   "metadata": {},
   "source": [
    "simple example of a custom callback that saves to disk (as Numpy arrays) the activations of every layer of the model at the end of every epoch, computedon the first sample of the validation st:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe919c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class ActivationLogger(keras.callbacks.Callback):\n",
    "    def set_model(self, model):\n",
    "        self.model = model # called by the parent model before training, to inform the callback of what model will be calling it\n",
    "        layer_outputs = (layer.output for layer in model.layers)\n",
    "        self.activations_model = keras.models.Model(model.input, layer_outputs) # modelinstance that returns the activations of every layer\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is None:\n",
    "            raise RuntimeError('Requires validation_Data.')\n",
    "            validation_sample = self.validation_data[0][0:1] # obtain the first input sample of the validation data\n",
    "            activations = self.activations_model.predict(validation_sample)\n",
    "            f = open('activation_at_epoch' + str(epoch) + '.npz', 'w') # saces arrays to disk\n",
    "            np.savez(f, activations)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ac034b",
   "metadata": {},
   "source": [
    "### Introduction to TensorBoard: the Tensorflow Visualization Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacb522f",
   "metadata": {},
   "source": [
    "![The Loop of Progress](loop_progress.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b36e7",
   "metadata": {},
   "source": [
    "tensorboard gives access to several neat features:\n",
    "- visually monitorirrng metrics during training\n",
    "- visually model architecture\n",
    "- visualizing histograms of activations and gradients\n",
    "- exploring embeddings in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d24831d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "40/40 [==============================] - 1s 24ms/step - loss: 0.4711 - acc: 0.8091 - val_loss: 0.3463 - val_acc: 0.8806\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.2735 - acc: 0.9094 - val_loss: 0.2893 - val_acc: 0.8886\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.2078 - acc: 0.9293 - val_loss: 0.3599 - val_acc: 0.8554\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1723 - acc: 0.9412 - val_loss: 0.2775 - val_acc: 0.8928\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1452 - acc: 0.9510 - val_loss: 0.2830 - val_acc: 0.8930\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1247 - acc: 0.9576 - val_loss: 0.3045 - val_acc: 0.8870\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1065 - acc: 0.9651 - val_loss: 0.3182 - val_acc: 0.8858\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0921 - acc: 0.9699 - val_loss: 0.3499 - val_acc: 0.8828\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.0778 - acc: 0.9757 - val_loss: 0.3695 - val_acc: 0.8816\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.0662 - acc: 0.9804 - val_loss: 0.4109 - val_acc: 0.8746\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0561 - acc: 0.9845 - val_loss: 0.4156 - val_acc: 0.8742\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.0496 - acc: 0.9858 - val_loss: 0.4809 - val_acc: 0.8724\n",
      "Epoch 13/20\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.0390 - acc: 0.9899 - val_loss: 0.4720 - val_acc: 0.8730\n",
      "Epoch 14/20\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.0333 - acc: 0.9912 - val_loss: 0.5433 - val_acc: 0.8718\n",
      "Epoch 15/20\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.0270 - acc: 0.9940 - val_loss: 0.5395 - val_acc: 0.8690\n",
      "Epoch 16/20\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0213 - acc: 0.9957 - val_loss: 0.5670 - val_acc: 0.8710\n",
      "Epoch 17/20\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.0178 - acc: 0.9963 - val_loss: 0.6153 - val_acc: 0.8672\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.0139 - acc: 0.9970 - val_loss: 0.6460 - val_acc: 0.8678\n",
      "Epoch 19/20\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0122 - acc: 0.9976 - val_loss: 0.6761 - val_acc: 0.8684\n",
      "Epoch 20/20\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.0083 - acc: 0.9990 - val_loss: 0.7208 - val_acc: 0.8674\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join(\n",
    "    [reverse_word_index.get(i - 3, ' ') for i in train_data[0]])\n",
    "\n",
    "# encoding the integer sequences into a binary matrix\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "# the model definition\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# training the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='my_log_dir',\n",
    "        histogram_freq=1,\n",
    "        embeddings_freq=1,\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                   y_train,\n",
    "                   epochs=20,\n",
    "                   batch_size=512,\n",
    "                   validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
