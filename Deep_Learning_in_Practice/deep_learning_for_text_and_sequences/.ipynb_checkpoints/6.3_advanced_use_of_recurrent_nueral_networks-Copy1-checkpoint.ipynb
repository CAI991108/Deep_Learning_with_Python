{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f28131",
   "metadata": {},
   "source": [
    "advanced techniques for improving the performance and generalization power of recurrent neural networks:\n",
    "- recurrent dropout -- specific, built-in way to use dropout to fight overfitting in recurrent layers\n",
    "- stacking recurrent layers -- increases the representational power of the network (at the cost of higher computational loads)\n",
    "- bidirectional recurrent layers -- present the same information to a recurrent network in different ways, increasing accuracy and mitigating forgetting issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc2aed",
   "metadata": {},
   "source": [
    "### A Stock Price Forecasting Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac37e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "data_dir = 'C:/Users/jim_c/Desktop/Task/local_base/benchmark_index'\n",
    "fname = os.path.join(data_dir, '000300.SH.csv')\n",
    "\n",
    "df = pd.read_csv(fname, header=0, \n",
    "                 usecols=[2,3,4,5,6,7],\n",
    "                 names=[\"open\", \"high\", \"low\", \"close\", \"vol\", \"amount\"])\n",
    "df = df.reindex(index=df.index[::-1])\n",
    "\n",
    "output_path='C:/Users/jim_c/Desktop/Task/Literature/Deep_Learning_with_Python/Deep_Learning_in_Practice/deep_learning_for_text_and_sequences/HS300.csv'\n",
    "df.to_csv(output_path,sep=',',index=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697fcea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onspecting the data of the HS.300 dataset\n",
    "import os \n",
    "\n",
    "data_dir = 'C:/Users/jim_c/Desktop/Task/Literature/Deep_Learning_with_Python/Deep_Learning_in_Practice/deep_learning_for_text_and_sequences'\n",
    "fname = os.path.join(data_dir, 'HS300.csv')\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "\n",
    "print(header)\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing the data (indexerror but work)\n",
    "import numpy as np\n",
    "\n",
    "float_data = np.zeros((len(lines) - 1, len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i, :] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f179e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the closing price timeseries\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "close_price = float_data[:, 3]\n",
    "plt.plot(range(len(close_price)), close_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c50ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting the recent 250 days of closing price timeseries\n",
    "plt.plot(range(250), close_price[len(close_price) - 250:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aa2667",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98668ca2",
   "metadata": {},
   "source": [
    "exact formlation:\n",
    "- given data going as far back as lookback timesteps (a timestep is 1 trading day) and sampled every steps tiemsteps, can you predict the temperature in delay timesteps?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058696bd",
   "metadata": {},
   "source": [
    "- lookback = 60 (obeservations will go back 3 months)\n",
    "- steps = 5 (observations will be sampleed at one data point per week)\n",
    "- delay = 1 (target will be the next day in future)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f980a71",
   "metadata": {},
   "source": [
    "- preprocess the data to a format a neural network can ingest; normalize each timeseries independently so that they all take small values on a similar scale\n",
    "- write a python generator that takes the current arry of float data and yield batches of data from the recent past"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c368ce8",
   "metadata": {},
   "source": [
    "preprocess the data by subtracting the mean of each timeseries and dividing by the standard deviation; the first 4000 timestepds will be as the training daata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff89633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the data\n",
    "mean = float_data[:4000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:4000].std(axis=0)\n",
    "float_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69212525",
   "metadata": {},
   "source": [
    "the data generator yields a tuple (samples, targets), where samples is one batch of input data and targets is the corresponding arry of target temperatures, with the following arguments:\n",
    "- data -- the original array of floating-point data, which is normalized\n",
    "- lookack -- timesteps back the input data go\n",
    "- delay  -- timesteps in the future target\n",
    "- min_index and max_index -- indices in the data array that delimit which timesteps to draw from ; useful for keeping a segment of the data for validation and another for testing\n",
    "- shuffle -- whether to shuffle the samples or draw them in chronological order\n",
    "- batch_size = the number of samples per batch\n",
    "- steps = the period in timesteps, at which you sample data; set to 5 in order to draw one data point per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b67e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator yielding timeseries samples an their targets\n",
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "              shuffle=False, batch_size = 64, step=5):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                    i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "                    \n",
    "        samples = np.zeros((len(rows), \n",
    "                            lookback // step,\n",
    "                            data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5178ea1a",
   "metadata": {},
   "source": [
    "use the generator function to instantiate three generators: one for training, one for validation, and one for testing:\n",
    "- training -- the first 4000 timesteps\n",
    "- validation -- the following 300 timesteps\n",
    "- test -- the remaineder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f96f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the training, validation, and test generators\n",
    "lookback = 60\n",
    "step = 5\n",
    "delay = 1\n",
    "batch_size = 64\n",
    "\n",
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=4000,\n",
    "                      shuffle=True,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=4001,\n",
    "                    max_index=4300,\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "\n",
    "test_gen = generator(float_data,\n",
    "                     lookback=lookback,\n",
    "                     delay = delay,\n",
    "                     min_index=4301,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "val_steps = (4300 - 4001 - lookback)\n",
    "\n",
    "test_steps = (len(float_data) - 4301 - lookback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4969a1",
   "metadata": {},
   "source": [
    "### A Common-Sense, Non-Machine-Learning Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf239b6e",
   "metadata": {},
   "source": [
    "simply assume that the stock market series can safely be assumed to be continuous as well as periodical with a daily period, thus a common sense approach is to predict that the stock market series will be equal to its previous days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adbf1be",
   "metadata": {},
   "source": [
    "evaluate with the mean absolute error metric: np.mean(np.abs(preds - targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cfda12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing thte common-sense baseline MAE\n",
    "def evaluate_navie_method():\n",
    "    batch_maes = []\n",
    "    for step in range(val_steps):\n",
    "        samples, targets = next(val_gen)\n",
    "        preds = samples[:, -1, 1]\n",
    "        mae = np.mean(np.abs(preds - targets))\n",
    "        batch_maes.append(mae)\n",
    "        print(np.mean(batch_maes))\n",
    "\n",
    "evaluate_navie_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9235ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the mae back to a celsius error\n",
    "celsius_mae = 0.29 * std[1]\n",
    "celsius_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9dccdd",
   "metadata": {},
   "source": [
    "### A Basic Machine-Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5082f56",
   "metadata": {},
   "source": [
    "it's useful to try simple, cheap machine-learning models (such as small, densely connected networks) before looking into complicated and computationally expensive models such as RNNs, which make sure any further complexity you throw at the problem is legitimate and delives real benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a34e420",
   "metadata": {},
   "source": [
    "note that the lack of activation function on the last Dense layer, which is typical for a regression problem; using mae as the loss beacause you evaluate on the  exact same data and with the exact same metric with the common-sense approach, the results will be directly comparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dac805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluating a densely connected model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=128,\n",
    "                              epochs=10,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240dcd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19d7d4",
   "metadata": {},
   "source": [
    "You may wonder, if a simple, well-performing model exists to go from the data to the targets (the common-sense baseline), why doesn’t the model you’re training find it and improve on it? Because this simple solution isn’t what your training setup is looking for. The space of models in which you’re searching for a solution—that is, your hypothesis space—is the space of all possible two-layer networks with the configuration you defined. These networks are already fairly complicated. When you’re looking for a solution with a space of complicated models, the simple, well-performing baseline may be unlearnable, even if it’s technically part of the hypothesis space. That is a pretty significant limitation of machine learning in general: unless the learning algorithm is hardcoded to look for a specific kind of simple model, parameter learning can sometimes fail to find a simple solution to a simple problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38a5654",
   "metadata": {},
   "source": [
    "### A First Recurrent Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63901361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluating a GRU-based model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=128,\n",
    "                    epochs=12,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c026ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512feeb",
   "metadata": {},
   "source": [
    "### Using Recurrent Dropout to Fight Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212da301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluating a dropout-regularized GRU-based model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, \n",
    "                     dropout=0.5,\n",
    "                     recurrent_dropout=0.5,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=128,\n",
    "                    epochs=10,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeceae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890746fa",
   "metadata": {},
   "source": [
    "### Stacking Recurrent Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e85b9",
   "metadata": {},
   "source": [
    "because the model is no longer overfitting but seem to have hit a performance bottleneck, its capacity should be increased "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e28454",
   "metadata": {},
   "source": [
    "increseing network capacity is typically done by increasing the number of units in the layers or addnig more layers; to stack recurrent layers on top of each other in keras, all intermediate layers should return their full sequence of output rahter than their outptu at thte last timestep; this is done by specifying 'return_sequence=True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57712c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluating a dropout-regularized, stacked GRU model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, \n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.5,\n",
    "                     input_shape=(None, float_data.shape[-1]),\n",
    "                     return_sequences=True))\n",
    "model.add(layers.GRU(64, activation='relu',\n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=128,\n",
    "                    epochs=16,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd518443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32db27",
   "metadata": {},
   "source": [
    "### Using Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e0347c",
   "metadata": {},
   "source": [
    "a bidirectional RNN is a common RNN variant taht can offer greater performance than a regualr RNN on certain tasks, frequently used in natural-language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888b5d9",
   "metadata": {},
   "source": [
    "- RNNs are notably order dependent, or time dependent: process the timesteps of their input sequence in order and shuffling or reversing the timesteps can completely change the representation the RNN extracts from the sequence\n",
    "- a bidirectional RNN exploits the orfer sensitivity of RNNs: it consists of using two regular RNNs, such as the GRU an LSTM layers , each of which processes the input sequence in one direction (chronologically and antichronologically), and then merging their representations\n",
    "- by processing a sequence both ways, a bidirectional RNN can catch patterns that may be overlookded by a unidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b96a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator yielding reversed timeseries samples an their targets\n",
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "              shuffle=False, batch_size = 64, step=5):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                    i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "                    \n",
    "        samples = np.zeros((len(rows), \n",
    "                            lookback // step,\n",
    "                            data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples[:, ::-1, :], targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a442c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the reversed training, validation, and test generators\n",
    "lookback = 60\n",
    "step = 5\n",
    "delay = 1\n",
    "batch_size = 64\n",
    "\n",
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=4000,\n",
    "                      shuffle=True,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=4001,\n",
    "                    max_index=4300,\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "\n",
    "test_gen = generator(float_data,\n",
    "                     lookback=lookback,\n",
    "                     delay = delay,\n",
    "                     min_index=4301,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "val_steps = (4300 - 4001 - lookback)\n",
    "\n",
    "test_steps = (len(float_data) - 4301 - lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c69b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluating a dropout-regularized, stacked GRU model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, \n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.5,\n",
    "                     input_shape=(None, float_data.shape[-1]),\n",
    "                     return_sequences=True))\n",
    "model.add(layers.GRU(64, activation='relu',\n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=128,\n",
    "                    epochs=16,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ca95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3599b8e1",
   "metadata": {},
   "source": [
    "the reversed-order GRU strongly underperforms even the common-sense baselinem, indicating that chronological processing is important to the success in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e5d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluating an LSTM using reversed sequences\n",
    "model = Sequential()\n",
    "model.add(layers.LSTM(32, \n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.5,\n",
    "                     input_shape=(None, float_data.shape[-1]),\n",
    "                     return_sequences=True))\n",
    "model.add(layers.LSTM(64, activation='relu',\n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=128,\n",
    "                    epochs=16,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d800bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f19e2",
   "metadata": {},
   "source": [
    "![How a Bidirectional RNN Layer Works](./bidir_rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a630533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluating a bidirectional LSTM\n",
    "model = Sequential()\n",
    "model.add(layers.Bidirectional(layers.LSTM(32, \n",
    "                                          dropout=0.2,\n",
    "                                          recurrent_dropout=0.5,\n",
    "                                          input_shape=(None, float_data.shape[-1]),\n",
    "                                          return_sequences=True)))\n",
    "model.add(layers.Bidirectional(layers.LSTM(64, activation='relu',\n",
    "                                          dropout=0.2,\n",
    "                                          recurrent_dropout=0.5)))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=128,\n",
    "                    epochs=16,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe9f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7841e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluating a bidirectional GRU\n",
    "model = Sequential()\n",
    "model.add(layers.Bidirectional(layers.GRU(32, \n",
    "                                         dropout=0.2,\n",
    "                                         recurrent_dropout=0.5,\n",
    "                                         input_shape=(None, float_data.shape[-1]),\n",
    "                                         return_sequences=True)))\n",
    "model.add(layers.Bidirectional(layers.GRU(64, activation='relu',\n",
    "                                         dropout=0.2,\n",
    "                                         recurrent_dropout=0.5)))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=128,\n",
    "                    epochs=16,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f554a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532fdc69",
   "metadata": {},
   "source": [
    "### Going Even Further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fdaf25",
   "metadata": {},
   "source": [
    "there are many other things to imporve performance on the temperature-forecasting problem:\n",
    "- adjust the number of units in each recurrent layer in the stacked setup, the current choice are largely arbitraty and thus probably suboptimal\n",
    "- adjust the learning rate used by the RMSprop optimizer\n",
    "- try using LSTM layers instead of GRU layers\n",
    "- try using a bigger densely connected regressor on top of the recurrent layers: a bigger Dense layer or even a stack of Dense layers\n",
    "- don't forget to eventually run the best-performing models (in terms of validation MAE) on the test set! otherwise, the architectures are overfitting to the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ff5835",
   "metadata": {},
   "source": [
    "![Markets and Machine Learning](./markets_and_ml.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in test_gen:\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    " \n",
    "if __name__=='__main':\n",
    "    #test image directory\n",
    "    dst_path = 'E:/datasets/Rcam-plusMelangerTaile/8KLSBackWindow/test'\n",
    "    #model path\n",
    "    model_file = \"D:/CF_new/piglin_alchemy/ckpt/tl_weights.40-0.7715.h5\"\n",
    "    batch_size = 8\n",
    " \n",
    "    # load model\n",
    "    model = load_model(model_file)\n",
    "    # generator image\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    " \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        dst_path,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "        )\n",
    " \n",
    "    labels = test_generator.class_indices #查看类别的label\n",
    "    #然后直接用predice_geneorator 可以进行预测\n",
    "    test_generator.reset()\n",
    "    pred = model.predict_generator(test_generator, verbose=1)\n",
    "    # 输出每个图像的预测类别\n",
    "    predicted_class_indices = np.argmax(pred, axis=1)\n",
    "    #测试集的真实类别\n",
    "    true_label= test_generator.classes\n",
    " \n",
    "    #使用pd.crosstab来简单画出混淆矩阵\n",
    "    import pandas as pd\n",
    "    table=pd.crosstab(predicted_class_indices,true_label,colnames=['predict'],rownames=['label'])\n",
    "    print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
