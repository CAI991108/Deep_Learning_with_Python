{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0044c14e",
   "metadata": {},
   "source": [
    "### A Brief History of Generative Recurrent Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b5167",
   "metadata": {},
   "source": [
    "![A Brief History of Generative Recurrent Netowrks](./history_grn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be9f0f",
   "metadata": {},
   "source": [
    "### How to Generate Sequence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d0b426",
   "metadata": {},
   "source": [
    "- the universal way to generate sequence data in deep learning is to train a network (usually an RNN or a convnet) to predict the next token or next few tokens in a sequence\n",
    "- as usual when working with text data, tokens are typically words or characters, and any network that can model the probaility of the next token given the previous ones is called a language model; a language model captures the latent space of language: its statistical structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a122ae3",
   "metadata": {},
   "source": [
    "once trained a language model, you can sample from it (generative new sequences): feed it an initial string of text (called conditioning data), ask it to generate the next character or the next word (you can even generate several tokens at once), add the generaed output back to the input data, and repeat the process many times "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb9bfd4",
   "metadata": {},
   "source": [
    "![The Process of Character-by-Character Text Generation Using a Language Model](./process_language_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94efc361",
   "metadata": {},
   "source": [
    "### The Importance of the Sampling Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c84b02",
   "metadata": {},
   "source": [
    "in order to control the amount of stochasticity in the sampling process, we'll introduce a parameter called the 'softmax temperature' that characterizes teh entropy of the probability distribution used for sampleing: it characterizes how surpirsing or predictable the choice of the next character will be "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbc6e0e",
   "metadata": {},
   "source": [
    "given a temperature value, anew probability distribution is computed from the original one (the softmax output of the model) by reweighting it in the following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1dcc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reweighting a probability distribution to a different temperature\n",
    "import numpy as np\n",
    "\n",
    "def reweight_distribution(original_distribution, temperature=0.5): \n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    return distribution / np.sum(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c647a1",
   "metadata": {},
   "source": [
    "![Different Reweightings of One Probability Distribution; Low Temperature = more deterministic, High Temperature = more random](./temperature.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161170c2",
   "metadata": {},
   "source": [
    "### Implementin Character-Level LSTM Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c74a9",
   "metadata": {},
   "source": [
    "##### 中文词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f35a02d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 467149 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('C:/Users/jim_c/Desktop/Task/Chinese-Word-Vectors/sgns.financial.bigram-char', encoding='utf8')\n",
    "for line in f:\n",
    "    while 1:\n",
    "        try:\n",
    "            line = f.readline()\n",
    "        except:\n",
    "            pass\n",
    "        if (line !=''):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:])\n",
    "            embeddings_index[word] = coefs\n",
    "        else:\n",
    "            break\n",
    "f.close()\n",
    "    \n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b6bff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['-0.356612', '-0.279084', '-0.126416', '0.628335', '0.551051',\n",
       "       '0.335221', '-0.880320', '0.223030', '0.112443', '0.001736',\n",
       "       '0.120411', '-0.108307', '-0.600671', '-1.378021', '0.335731',\n",
       "       '0.160973', '0.228364', '-0.403491', '-0.659103', '0.653530',\n",
       "       '0.832316', '-0.670925', '1.804369', '0.279993', '-0.859297',\n",
       "       '0.597914', '1.167754', '0.733485', '-0.073254', '-0.481956',\n",
       "       '-0.265198', '-0.642714', '0.292259', '-0.426623', '0.389194',\n",
       "       '-1.224456', '1.046738', '0.827610', '0.200318', '0.126715',\n",
       "       '-0.832044', '0.542205', '0.575616', '0.069012', '-0.315015',\n",
       "       '-0.285998', '-0.351620', '-0.336552', '-0.676042', '0.550532',\n",
       "       '-0.146298', '-0.496723', '-0.326674', '-0.568264', '0.609188',\n",
       "       '0.258278', '0.025612', '-0.440745', '-0.411169', '-0.797708',\n",
       "       '0.021110', '-0.259782', '0.104928', '-0.565511', '-0.208318',\n",
       "       '0.522938', '0.358395', '0.063634', '-0.514831', '0.461302',\n",
       "       '-0.202765', '1.007935', '-0.128045', '-0.200597', '0.699825',\n",
       "       '0.191186', '0.752622', '0.118923', '0.141373', '0.472534',\n",
       "       '-0.686810', '0.509377', '0.601225', '0.416077', '-1.154321',\n",
       "       '0.574146', '0.093303', '0.534050', '0.126546', '0.123581',\n",
       "       '-0.568871', '0.272417', '-0.094166', '-0.688704', '0.443307',\n",
       "       '0.116471', '-0.024292', '0.008454', '0.720977', '-0.222300',\n",
       "       '1.334513', '0.334181', '-0.108363', '-0.520661', '0.481063',\n",
       "       '-0.850965', '-0.798296', '-0.276995', '-0.168303', '-0.042145',\n",
       "       '-1.013349', '0.167011', '0.289586', '-0.017833', '-0.451011',\n",
       "       '-0.363980', '-1.437555', '0.711262', '0.881292', '0.004277',\n",
       "       '0.467635', '0.203373', '-0.482167', '-0.845683', '-0.862506',\n",
       "       '0.181405', '-0.643594', '-1.089529', '-0.215546', '-0.939297',\n",
       "       '0.207260', '0.898656', '-0.607439', '-1.471625', '0.232987',\n",
       "       '-0.608524', '-0.341070', '1.060244', '-0.441624', '-1.244156',\n",
       "       '-0.793635', '0.841815', '0.311678', '0.442363', '-0.960743',\n",
       "       '-0.455878', '0.376805', '0.223196', '-0.871802', '1.062400',\n",
       "       '0.030429', '-0.508885', '-1.204696', '-0.296914', '0.899972',\n",
       "       '-0.375893', '-0.198540', '0.078012', '0.779883', '-0.190195',\n",
       "       '-0.543163', '0.105647', '-0.121859', '-0.408496', '0.752114',\n",
       "       '0.214805', '0.044736', '1.012001', '-0.714741', '-0.226221',\n",
       "       '-0.567421', '-0.955905', '0.299946', '-0.064903', '-0.881341',\n",
       "       '-1.630528', '0.545430', '0.794040', '-0.704557', '-0.755416',\n",
       "       '0.055428', '-0.456441', '-0.006803', '0.074483', '-0.861373',\n",
       "       '0.003773', '0.390445', '-1.180889', '0.253424', '0.412670',\n",
       "       '0.480373', '0.294997', '-0.213806', '0.715190', '-0.222579',\n",
       "       '-0.160571', '-0.924469', '0.334117', '0.440287', '0.623278',\n",
       "       '-0.798917', '0.396702', '-1.046968', '0.598157', '-0.206756',\n",
       "       '-0.187010', '1.246261', '0.468288', '0.374381', '-0.026683',\n",
       "       '-0.436625', '-0.473349', '-0.226611', '0.286294', '-0.495439',\n",
       "       '0.339577', '-1.368178', '0.067698', '-0.409403', '0.989202',\n",
       "       '0.596417', '0.627466', '0.471229', '0.725820', '0.420599',\n",
       "       '0.837886', '-0.618003', '0.795781', '0.046824', '-0.441706',\n",
       "       '1.046240', '0.000166', '0.905193', '0.085687', '0.420190',\n",
       "       '-0.793182', '-0.734568', '0.033796', '0.725366', '0.187366',\n",
       "       '-0.274951', '-0.267193', '-0.961724', '-0.372609', '-0.364584',\n",
       "       '0.416382', '0.051992', '0.940809', '-0.563686', '-0.199795',\n",
       "       '0.277456', '-0.511945', '-0.576579', '0.689688', '-0.468430',\n",
       "       '1.110620', '0.229844', '-0.371040', '0.346560', '-0.234276',\n",
       "       '-0.335736', '-0.068920', '0.380289', '-0.300158', '-0.271956',\n",
       "       '0.774494', '0.687891', '-1.226811', '-0.145045', '0.370731',\n",
       "       '0.354658', '-0.544873', '-0.800991', '-0.203745', '-0.011720',\n",
       "       '-0.107522', '0.584750', '0.023451', '-0.066095', '0.041301',\n",
       "       '-0.191000', '-0.491359', '0.352803', '0.914229', '-1.449526',\n",
       "       '0.269135', '0.136014', '0.337566', '-0.355366', '0.460050',\n",
       "       '-0.265601', '0.203003', '0.089268', '-0.727586', '0.610628',\n",
       "       '0.415442', '-0.089639', '0.613505', '0.655002', '0.239912'],\n",
       "      dtype='<U9')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['惠丰']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d97195",
   "metadata": {},
   "source": [
    "##### Building the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e13a55",
   "metadata": {},
   "source": [
    "single LSTM layer followed by a Dense classifier and sorfmax over all possible characters; but note taht recurrent neural networks aren't the only way to do sequence data generation; 1D convnets also have proven extremely successul at this task in recent times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97803e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single-layer LSTM model for next-character prediction\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(100, len(embeddings_index))))\n",
    "model.add(layers.Dense(len(embeddings_index), activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58925f",
   "metadata": {},
   "source": [
    "##### Training the Language Model and Sampling from it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8357fe3e",
   "metadata": {},
   "source": [
    "given a trained model and a seed text snippet, you can generate new text by doing the following repeatedly:\n",
    "- draw from the model a probability distribution for the text character, given the generated text available so far\n",
    "- reweight the distribution to a certain temperature\n",
    "- sample the next character at random according to the reweighted distribution\n",
    "- add the new charavter at the end of the available text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94ebc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to sample the next character given the model's predictions\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e762ccd5",
   "metadata": {},
   "source": [
    "the loop repeatedly trains and generates text, using a range of different temperature after evey epoch; this allows to see how the generated text evolves as the model begins to converge, as well as the impact of temperature in the sampling strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33cc6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text-generation loop\n",
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 60):\n",
    "    print('epoch', epoch)\n",
    "    model.fit(x, y, batch_size=128, epochs=1)\n",
    "    generated_text = []\n",
    "    \n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('-------temperature:'. temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, 100, 467149))\n",
    "            for t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
