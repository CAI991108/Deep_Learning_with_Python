{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d2c3ef",
   "metadata": {},
   "source": [
    "[problem definition] + [evaluation]+ [feature engineering] + [fighting overfitting]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f4c2a9",
   "metadata": {},
   "source": [
    "##### Defining the Problem and Assembling a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b304ff",
   "metadata": {},
   "source": [
    "- define the problem:\n",
    "    - the input data and the prediction -- you can only learn to predict something if you have available training data\n",
    "    - the type of problem -- binary classification? multiclass-classification?s scalar regression? vector regression? multiclass, multilabel classification? something else like clustering, generation, or reinforcement learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58610538",
   "metadata": {},
   "source": [
    "be aware of the hypotheses at this stages:\n",
    "- your output can be predicted given your inputs\n",
    "- your available data is sufficiently informative to learn the relationship between inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1047d6d",
   "metadata": {},
   "source": [
    " not all problems can be solved; because you've assembled examples of inputs X and targets Y doesn't mean X contains enough information to predict Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ee22b",
   "metadata": {},
   "source": [
    "machine learning can only be used to memoriza patterns that are present in the training data, you can only recognize what you've seen before. Using machine learning trained on past data to predict the future is making the assumption that the future will behave like the past. that often isn't the case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a80e8c2",
   "metadata": {},
   "source": [
    "##### Choosing a Measure of Success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6696b",
   "metadata": {},
   "source": [
    "your metric for success will guide the choice of a loss function: what your model will optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a6e78",
   "metadata": {},
   "source": [
    "- for balanced-classification problem, where every class is equally likely, accuracy and 'area udner the receiver operating charactureistic curve' (ROC AUC) are common metrics\n",
    "- for class-imbalanced problem, you can use precision and recall\n",
    "- it isn't uncommon to have to define your own custom metric by which to measure success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46584a4d",
   "metadata": {},
   "source": [
    "##### Deciding on An Evaluation Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb7fbf3",
   "metadata": {},
   "source": [
    "once you know what you're aiming for, establish how you will measure your current progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f11b79",
   "metadata": {},
   "source": [
    "three common evaluation protocols previously reviewed:\n",
    "- a hold-out validation set -- the way to go when you have plenty of data\n",
    "- k-fold cross-validation -- the right choice when you have too few samples for hold-out validation \n",
    "- iterated k-fold validation -- for performing highly accurate model evaluation when litte data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5283d1ca",
   "metadata": {},
   "source": [
    "##### Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f6fcd",
   "metadata": {},
   "source": [
    "format your data in a way that can be fed into a machine-learning model -- assuming a deep neural network: \n",
    "- the data should be formatted as tensors\n",
    "- the values taken by these tensors should usually be scaled to small values: i.e., in the [-1,1] range or [0,1]range\n",
    "- if different features take values in different ranges (heterogeneous data), then the data should be normalized\n",
    "- you may want to do some feature engineering, especially for small data problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab970863",
   "metadata": {},
   "source": [
    "##### Developming a Model that Does Better Than a Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189e66e",
   "metadata": {},
   "source": [
    "note that it's not always possible to achieve statistical power, if you can't beat a random baseline after tying multiple reasonable architectures, it may be that the answer isn't present in the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ce2f0",
   "metadata": {},
   "source": [
    "- you hypothesize that your outputs can be predicted given your inputs\n",
    "- you hypothesize that the available data is sufficiently informative to learn the relationship between inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2528d140",
   "metadata": {},
   "source": [
    "assuming that things go well, you need to make three key choices to build your first working model:\n",
    "- last-layer activation -- this extablishes useful constraints on the network's output\n",
    "- loss function -- this should match the type of problem you're trying to solve\n",
    "- optimization configuration -- what optimizer and what will its learning rate be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f79b7f8",
   "metadata": {},
   "source": [
    "regarding the choice of a loss function, note that it isn't always possile to directly optimize for the metric that measures success on a problem, sometimes there is no easy way to turn a metric into a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53362551",
   "metadata": {},
   "source": [
    "loss function need to be computable given only a mini-batch of data (ideally, a loss function should be computable for as little as a single data point) and must be differentiable (otherwise, you can't use backpropagation to train your network)\n",
    "- for instance, the widely used classification metric ROC AUC can't be directly optimized. hence it's common to optimize for a proxy metric of ROC AUC, such as crossentropy. In general, you can hope that the lower the crossentropy gets, the higher the ROC AUC will be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbade91",
   "metadata": {},
   "source": [
    "![Choosing the Right Last-Layer Activation and Loss Function for Your Model](./c_l_layer_actv_and_loss_fctn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b32ada",
   "metadata": {},
   "source": [
    "##### Scaling Up: Developing a Model that Overfits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
