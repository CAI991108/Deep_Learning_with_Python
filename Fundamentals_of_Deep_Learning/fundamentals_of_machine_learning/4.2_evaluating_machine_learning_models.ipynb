{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ca7b99",
   "metadata": {},
   "source": [
    "the reason not to evaluate the models on the same data they were trained on quickly became evident: after just a few epochs, all three models began to overfit, their performance on never-before-seen data started stalling compared to their performance on the training data, which always improves as training progresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f9e83",
   "metadata": {},
   "source": [
    "in machine learning, the goal is to achieve models taht generalise that perform well on never-before-seen data, and overfitting is the central obstacle, the model can only control that which can be boserved, so it is crucial to be able to reliably measure the generalization power of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcac7fd",
   "metadata": {},
   "source": [
    "##### Training, Validation and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96831622",
   "metadata": {},
   "source": [
    "- splitting the available data into three sets:\n",
    "    - training sets\n",
    "        - training the data\n",
    "    - validation\n",
    "        - evaluate the model\n",
    "    - test\n",
    "        - test it one final time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824618e",
   "metadata": {},
   "source": [
    "developing a model always involves tuning its configuration: (hyperparameters to distinguish them from the parameters, which are the network's weights), the validation set tuning as a feedback signal the perfoemance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fae79e2",
   "metadata": {},
   "source": [
    "in essence, the tuning is a form of learning: a search for good configuration in some parameter space, as a result, tuning the configuration of the model based on its performance on the validation set can quickly result in 'overfitting to the validation set', even though the model is never directly trained on it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1356e2",
   "metadata": {},
   "source": [
    "central to this pehnomenon is the notion of information leaks, every time tune a hyperparameter of the model based on the model's performance on the validation set, some information about the validation data leadks into the model\n",
    "- for one tuning, one parameter, very few bits of information will leak, and the validation set will remain reliable to evaluate the model\n",
    "- for repeating, running one experiment, evaluating on the validation set, and modifying the model as a result, then it will leak an increasingly significant amount of informaiton about the validation set into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568bbe8",
   "metadata": {},
   "source": [
    "the model will end up that performs artificially well on the validation data, but not the test set, otherwise the measure of generalization will be flawed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f2514b",
   "metadata": {},
   "source": [
    "##### Three Classic Evaluation recipes: \n",
    "- simple hold-out validation\n",
    "- k-fold validation\n",
    "- iterated k-fold validation with shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21f14e",
   "metadata": {},
   "source": [
    "##### Simple Hold-Out Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa3f1c",
   "metadata": {},
   "source": [
    "- set apart ome fraction of the data as test set\n",
    "- train on the remaining data\n",
    "- evaluate on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f8ecff",
   "metadata": {},
   "source": [
    "![simple_hold_out_validation_split](./h_o_val_split.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be89a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold-out validation\n",
    "\n",
    "num_validation_samples = 10000\n",
    "\n",
    "np.random.shuffle(data) # shuffling the data is usually appropiate\n",
    "\n",
    "validation_data = data[:num_validation_samples] # defines the validation set\n",
    "data = data[num_validation_samples:]\n",
    "\n",
    "training_data = data[:] # defines the training set\n",
    "\n",
    "model = get_model()\n",
    "model.train(training_data) # trains a model on the training data\n",
    "validation_score = model.evaluate(validation_data) # and evaluates it on the validation data\n",
    "\n",
    "# at this point you can tune the model\n",
    "# retrain it, evaluate it, tune it again\n",
    "\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data, \n",
    "                            validation_data]))\n",
    "test_score = model.evaluate(test_data)\n",
    "\n",
    "# once you've tuned the hyperparameters, \n",
    "# it's common to train the final model from scratch on all non-test data available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a60fcf",
   "metadata": {},
   "source": [
    "flaw:\n",
    "- if little data is available, then the validation and test sets may contain too few samples to be statistically representative of the data at hand\n",
    "- if different random shuffling rounds of the data before splitting end up yielding very different measures of model performance\n",
    "- k-fold validation and iterated k-fold validation are two ways to address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf01a2e",
   "metadata": {},
   "source": [
    "##### K-Fold Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf5b954",
   "metadata": {},
   "source": [
    "- split the data into K partitions of equal size\n",
    "- for each partition i, train a model on the remaining k-1 partition\n",
    "- and evaluate it on partition i\n",
    "- your final score is then the averages of the k scores obtained\n",
    "this methond is helpful when the performance of the model shows significant variance based on the train-test split, and like hold-out validation, this method doesn't exempt you from using a distinct validation set for model calirbation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396bcdd5",
   "metadata": {},
   "source": [
    "![three_fold_validation](./3_f_vali.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross-validation\n",
    "\n",
    "k = 4\n",
    "num_validation_samples = len(data) // k\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_score = []\n",
    "for fold in range(k):\n",
    "    validation_data = data[num_validation_samples * fold: # select the validation data partition\n",
    "     num_validation_samples * (fold + 1)]\n",
    "    training_data = data[:num_validation_samples * fold] + \n",
    "     data[num_validation_samples * (fold + 1):] # use the remainder of the data as training data, \n",
    "                                                # note the + is list concatenation not summation  \n",
    "    model = get_model() # create a brand new instance of the model (untrained)\n",
    "    model.train(training_data)\n",
    "    validation_score = model.evaluate(validation_data)\n",
    "    validation_scores.append(validation_score)\n",
    "\n",
    "validation_score = np.average(validation_scores) # validation score: the averagee of the validation scores of k-folds\n",
    "    \n",
    "model = get.model() # trains the final model on all non-test data available\n",
    "model.train(data)\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e068cf",
   "metadata": {},
   "source": [
    "##### Iterated K-Fold Validation with Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1ba4e",
   "metadata": {},
   "source": [
    "- for situations in which have relatively little data available but need to evaluate the model as precisely as possible (extremly helpful in kaggle):\n",
    "    - applying k-fold validation multiple times\n",
    "    - shuffling the data every time before splitting it k ways\n",
    "    - the final scores is the average of the scores obtained at each run of k-fold validation\n",
    "    - training and evaluating P x K models (where P is the number of iterations), which can be expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b727f00",
   "metadata": {},
   "source": [
    "##### Things to Keep in Mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f268a",
   "metadata": {},
   "source": [
    "when choosing an evaluation protocol:\n",
    "- data representativeness\n",
    "    - both training set and test set to be representative \n",
    "        - for instance, when classifying images of digits, and starting from an array of samples whihc ordered by their class, this seems like a ridiculous mistake, but it's surprisingly common\n",
    "        - usually should randomly shuffle the data before splitting it into training and test sets\n",
    "- the arrow of time\n",
    "    - if trying to predict the future given the past (i.e. tomorrow's weather, stock movements, and so on) do not randomly shuffle before splitting it, to avoid temporal leak\n",
    "        - model will effectively be trained on data from the future\n",
    "        - always make sure all data in test set is posterior to the data in the training set\n",
    "- redundancy in the data\n",
    "    - some data points appear twice (fairly common with real world data), then shuffling the data and splitting it into a training set and a validation set will result in redundancy between the training and validation sets, in effect, it is testing on part of training data, which is the worst thing you can do\n",
    "    - make sure training set and validation set are disjoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
