{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5d7fb1",
   "metadata": {},
   "source": [
    "many data-preprocessing and feature-engineering techniques are domain specific (i.e., specific to text data or image data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b38a14",
   "metadata": {},
   "source": [
    "##### Data Preprocessing for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d46eb",
   "metadata": {},
   "source": [
    "making the raw data more amenable to neural networks: vectorization, normalization, handling missing values and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1c20f0",
   "metadata": {},
   "source": [
    "VECTORIZATION:\n",
    "- all inputs and targets must be tensors of floating-point data (or in specific cases, tensors of integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64dfef",
   "metadata": {},
   "source": [
    "VALUE NORMALIZATION:\n",
    "- in general, it isn't safe to feed into a neural network data that takes relatively large values\n",
    "    - for example, multidigit integers, which are much larger than the initial values taken by the weights of a network\n",
    "    - or data that is heterogeneous (for example data where one feature is in the range 0-1 and another is in the range 100-200)\n",
    "- large gradient updates will prevent the network from converging\n",
    "- the data should have the following characteristics:\n",
    "    - take small value -- typically 0-1 range\n",
    "    - be homogenous -- features should take values in roungly the same range\n",
    "    - common stricter normalizastion:\n",
    "        - normalize each feature independently to have a mean of 0\n",
    "        - normalize each feature independently to have a standard deviation of 1\n",
    "        - see below numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e410bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming x is a 2D data matrix of shape (samples, features)\n",
    "x -= x.mean(axis=0)\n",
    "x /= x.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f0dad5",
   "metadata": {},
   "source": [
    "HANDLING MISSING VALUES:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ddb14c",
   "metadata": {},
   "source": [
    "- in general, with neural networks, it's safe to tinput missing values as 0\n",
    "    - with the condition that 0 isn't already a meaningful value\n",
    "    - the network will learn from exposure to the data that value 0 means 'missing data' and will start ignoring the value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ddc55",
   "metadata": {},
   "source": [
    "note:\n",
    "- if missing values are expecting in the test data, but the network was trained on data without any missing values, the network won't have learned to ignore missing values\n",
    "- one should artificially generate training samples with missing entreis (drop some)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed80f7f",
   "metadata": {},
   "source": [
    "##### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119242d1",
   "metadata": {},
   "source": [
    "to make the algorithm work better by applying hardcoded (non-learned) transformations before it goes into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ea0de",
   "metadata": {},
   "source": [
    "![Feature_Engineering_for_Reading_the_Time_on_a_Clock](./f_eng_time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ce407",
   "metadata": {},
   "source": [
    "- before deep learning, feature engineering used to be critical\n",
    "    - classical shallow algorithms didn't have hypothesis spaces rich enough to learn useful features by themselves\n",
    "    - before the invention of convolutional nerual network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d99497e",
   "metadata": {},
   "source": [
    "modern deep leraning removes the need for most feature engineering, because neural networks are capable of automatically extracting useful features from raw data, still notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d05c81",
   "metadata": {},
   "source": [
    "- good features allow to solve problems more elegantly while using fewer resources\n",
    "- good features let solve a problem with far less data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6739b1",
   "metadata": {},
   "source": [
    "##### Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e243e7",
   "metadata": {},
   "source": [
    "when converting the model, the performance on the held-out validation data always peaked after a few epochs and then began to degrade: the model quickly strat to overfit to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ffd86",
   "metadata": {},
   "source": [
    "the fundamental issue in machine learning is teh tesion between optimization and generalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef22796",
   "metadata": {},
   "source": [
    "- 'optimization' refers to the process of adjusting a model to get the best performance opssible on the training data\n",
    "- 'generalization' refers to how well the trained model perfoms on data it has never seen before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e566233",
   "metadata": {},
   "source": [
    "at the begining of training, optimization and generalization are correlated -- 'under fit'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68039419",
   "metadata": {},
   "source": [
    "after a certain number of iterations on teh training data, generalization stops improving, and validation metrics stall and then begin to degrade -- 'overfit' (begining to learn patterns taht are specific to the traiing data but misleading or irrelevant to the new data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c93b9a",
   "metadata": {},
   "source": [
    "'the best solution is to get more training data' -- a model trained on more data will naturally generalize better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90dea93",
   "metadata": {},
   "source": [
    "'the next-best solution is to modulate the quantity of information taht the model is allowed to store or to add constraints on what information it's allowed to store'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e44a50",
   "metadata": {},
   "source": [
    "the processing of fighting overfitting this way is called 'regluarization'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad1af75",
   "metadata": {},
   "source": [
    "##### Reducing the Network's Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80044fb0",
   "metadata": {},
   "source": [
    "the simplest way to prevent overfitting is to reduce the size of the model: the number of learnable parameters in the model (which is determined by the number of layers and teh number of units per layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb62c1",
   "metadata": {},
   "source": [
    "in deep learning, the number of learnable parameters referred to as the model's 'capacity', there is a compromise to be found between 'too much capacity and not enough capacity'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51439690",
   "metadata": {},
   "source": [
    "there is no magical formula to determine the right number of layers or the right size for each layer -- evaluate an arry of different architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ece84d",
   "metadata": {},
   "source": [
    "the general workflow to find an appropriate model size is to start with relatively few layers and parameters, and increase the size of the layers or add new layers until see diminishing returns with regard to validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376b53b",
   "metadata": {},
   "source": [
    "the movie-review classification network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e93951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 13:38:29.755554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 13:38:29.831760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 13:38:29.831974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 13:38:29.832735: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 13:38:29.835737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 13:38:29.835933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 13:38:29.836042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 13:38:30.801312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 13:38:30.801590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 13:38:30.801858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 13:38:30.802065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1893 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# original model\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb79f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version of the model with lower capacity\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bee578",
   "metadata": {},
   "source": [
    "![Effect of Model Capacity on Validation Loss: Trying a Smaller Model](./m_cpcty_val_s.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31c154ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version of the model with hifgher capacity\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84566123",
   "metadata": {},
   "source": [
    "![Effect of Model Capacity on Validation Loss: Trying a Bigger Model](./m_cpcty_val_b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82016d09",
   "metadata": {},
   "source": [
    "- the bigger network gets its training loss near zero very quickly\n",
    "- the more capacity the network has, the more quickly it can model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large difference between the traiing and validation loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d682fbc",
   "metadata": {},
   "source": [
    "![Effect of Model Capacity on Training Loss: Trying a Bigger Model](./m_cpcty_train_b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043c4493",
   "metadata": {},
   "source": [
    "##### Adding Weight Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d4fbbc",
   "metadata": {},
   "source": [
    "- Occam's Razor: \n",
    "    - given two explanations for something, the explanation most likely to be correct is the simplest one -- the one that makes fewer assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2c61ab",
   "metadata": {},
   "source": [
    "simpler models are less likely to overfit tahn complex one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bf56ae",
   "metadata": {},
   "source": [
    "- weight regularization: (adding to the loss function of the network a cost associated with having large weights)\n",
    "    - a common way to mitegate overfitting is to put constraints on the complexity of a network by forcing its weights to make only small values, which makes the distribution of weight values more regular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf51501",
   "metadata": {},
   "source": [
    "- L1 regularization: (L1 norm of the weights)\n",
    "    - the cost added is proportional to the absolute value of the weight coefficients \n",
    "- L2 regularization: (L2 norm of the weights) (weight decay)\n",
    "    - the cost added is proportional to the squre of the value of the weight coeefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154f0f0",
   "metadata": {},
   "source": [
    "L2 weight regularization to the movie-review classification network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "607d1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding L2 weight regularization to the model\n",
    "\n",
    "from keras import regularizers \n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                       activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                       activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9f65c",
   "metadata": {},
   "source": [
    "l2(0.001) means every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value to the total loss of the network (note taht because this penalty is only added at training time, the loss for this network will be much higher at training than at test time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf84e7",
   "metadata": {},
   "source": [
    "![Effect of L2 Weight Regularization on Validation Loss](./l2_reg_val.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e5c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different weight regularizers available in keras\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "regularizers.l1(0.001)\n",
    "\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a255842",
   "metadata": {},
   "source": [
    "##### Adding Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe03e1e",
   "metadata": {},
   "source": [
    "dropout is one of the most effective and most commonly used regularization techniqus for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e7f79",
   "metadata": {},
   "source": [
    "dropout, applied to a layer, consists of randomly 'dropping out' (setting to zero) a number of output features of the layer during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb734ed",
   "metadata": {},
   "source": [
    "the 'dropout rate' is the fraction of the features that are zeroed out; instead, the layer's output values are scaled down by a factor equal to the dropout rate, to balance for the fact that more units are active than at training time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f60ac",
   "metadata": {},
   "source": [
    "consider a numpy matrix constaining the output of a layer -- 'layer_output' of shape (batch_size, features); and zero out at random a fraction of the values in the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ffcfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at training time, drops out 50% of the units in the output at test\n",
    "layer_output *= np.random.randint(0, high=2. size=layer_output.shape)\n",
    "layer_output *= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6914c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at test time, scaling up rather scaling down\n",
    "layer_output *= np.random.randint(0, high=2, size=layer_output.shape)\n",
    "layer_output /= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e95a6",
   "metadata": {},
   "source": [
    "![Dropout](./50%_dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d9cc4",
   "metadata": {},
   "source": [
    "introducing noise in the output values of a layer can break up happenstance patterns that aren't significant, which the network will start memorizing if no noise is present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9860ff66",
   "metadata": {},
   "source": [
    "dropout layer in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36768ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding dropout to the IMDB network\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b5525",
   "metadata": {},
   "source": [
    "![Effect of Dropout on Validation Loss](./ef_dropout_val_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1fd05d",
   "metadata": {},
   "source": [
    "- recap: most common ways to prevent overfitting in neural network\n",
    "    - get more training data\n",
    "    - reduce the capacity of the network\n",
    "    - add weight regularization\n",
    "    - add dropout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
