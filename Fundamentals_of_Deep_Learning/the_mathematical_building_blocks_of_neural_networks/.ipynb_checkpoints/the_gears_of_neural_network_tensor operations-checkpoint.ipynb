{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4168961e",
   "metadata": {},
   "source": [
    "all transformations learned by deep neural networks can be reduced to a handful of 'tensor operations' applied to tensors of numeric data, much as any computer program can be ultimately reduced to a small set of binary operations on binary inputs (AND, OR, NOR, and so on)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42656f4c",
   "metadata": {},
   "source": [
    "In the initial example. the network was built by stacking 'Dense' layers on top of each other. A Keras layer instance looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2082c6d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m512\u001b[39m, activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "keras.layers.Dense(512, activation = 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f40e2f",
   "metadata": {},
   "source": [
    "This layer ca be interpreted as a function, which takes as input a 2D tensor and returns another 2D tensor - a new representation for the input tensor. Specifically, the funciton is as follows (where 'W' is a 2D tensor and 'b' is a vector, both attributes of the layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd58d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = relu(dot(W, input) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b033c03",
   "metadata": {},
   "source": [
    "Unpacking the three tensor operations here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b5c6e",
   "metadata": {},
   "source": [
    "- dot product (dot) between the input tensor and a tensor named 'W'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83bb6fe",
   "metadata": {},
   "source": [
    "- an addition (+) between the resulting 2D tensor and a vector (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8af6574",
   "metadata": {},
   "source": [
    "- a 'relu' operation .relu(x) is max(x,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c87c17",
   "metadata": {},
   "source": [
    "##### Element-wise operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db77e56b",
   "metadata": {},
   "source": [
    "The 'relu' operation and addition are 'element-wise' operations: operations that are applied independently to each entry in the tensors being considered. This means these operations are highly amenable to massively parallel implementations (vectorised implementations, a term that comes from the vector processor supercomputer architecture from the 1970-1990 period). To write a navie python implementation of an element-wise opearation, we can use a 'for loop', as in this naive implementaion of an element wise 'relu' operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_relu(x):\n",
    "    assert len(x.shape) == 2    # x is a 2D numpy tensor\n",
    "    \n",
    "    x = x.copy()     # avoid overwriting the input tensor\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963495e",
   "metadata": {},
   "source": [
    "the same for addition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2     # x and y are 2D Numpy tensors\n",
    "    assert x.shape == y.shape\n",
    "    \n",
    "    x = x.copy()     # avoid overwriting the input tensor\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j] += y[i,j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2073204d",
   "metadata": {},
   "source": [
    "On the same principle, the element-wise multiplication, subtraction and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463026ad",
   "metadata": {},
   "source": [
    "In practice, when dealing with Numpy arrays, these operations are available as well-optimised built-in Numpy functions, which themselves delegate the heavy lifting to a Basic Linear Algebra Subprograms (BLAS) implementation if you have one installed (which you should). BLAS are low-level, highly parallel, efficient tensor manipulation routines that are typically implemented in Fortran or C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c45b678",
   "metadata": {},
   "source": [
    "So in Numpy, the following element-wise operation will be blazing fast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8578a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "z = x + y\n",
    "\n",
    "z = np.maximum(z, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98324a5",
   "metadata": {},
   "source": [
    "##### Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5803d5",
   "metadata": {},
   "source": [
    "'naive_add' only supports the addtion with identical shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bbc8c7",
   "metadata": {},
   "source": [
    "Brocasting process addtion when the shapes of the two tensors being added differ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b1d0a",
   "metadata": {},
   "source": [
    "Assuming no ambiguity, the smaller tensor will be broadcasted to match the shape of the larger tensor. Boardcasting consists of two steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1cd433",
   "metadata": {},
   "source": [
    " - axes (called 'broadcast axes') are added to the smaller tensor to match the 'ndim' of the larger tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eceb5b",
   "metadata": {},
   "source": [
    "- the smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78302854",
   "metadata": {},
   "source": [
    "Example of navie implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba3214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add_matrix_and_vector(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in  range(x.shape[1]):\n",
    "            x[i, j] += y[j]\n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272278ca",
   "metadata": {},
   "source": [
    "Example of 'element-wise maximum operation' via broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3612739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.random((64, 3, 32, 10))\n",
    "y = np.random.random((32, 10))\n",
    "\n",
    "z = np.maximum(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd4b00",
   "metadata": {},
   "source": [
    "##### Tensor dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1d6c6c",
   "metadata": {},
   "source": [
    "- 'dot operation' ('tensor product') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e782d8c",
   "metadata": {},
   "source": [
    "- Contrary to element-wise operations, it combines entries in the input tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c85416",
   "metadata": {},
   "source": [
    "'dot' operator in Numpy and Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc5eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "z = np.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957da2a4",
   "metadata": {},
   "source": [
    "'dot' (.) in mathematical notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x . y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d07d783",
   "metadata": {},
   "source": [
    "Example of 'dot' operation of two vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84fd5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    z = 0 \n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f002be",
   "metadata": {},
   "source": [
    "the 'dot' product between two vectors is a scalar and that only vectors with the same number of elements are compatible for a 'dot' product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be77eb2",
   "metadata": {},
   "source": [
    "the 'dot' product between a matrix [x] and a vector [y] returns a vector where the coefficients are the 'dot' products between the [y] and the rows of [x]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a906602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) ==1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25472ca8",
   "metadata": {},
   "source": [
    "alternatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4281e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2425c",
   "metadata": {},
   "source": [
    "as soon as one of the two tensors has an 'ndim' greater than 1, 'dot' is no longer symmetric, which is to say that 'dot(x,y)' isn't the same as 'dot(y,x)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af7b96a",
   "metadata": {},
   "source": [
    " a 'dot' product generalises to tensors with an arbitrary number of axes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3251f36",
   "metadata": {},
   "source": [
    "the 'dot' product of two matrices [x] and [y] (dot(x,y)) if and only if 'x.shape[1] == y.shape[0]', the result is a matrix with shape (x.shape[0], y.shape[1]), where the coefficients are the vector products between the rows of [x] and the columns of [y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7686e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d82fa",
   "metadata": {},
   "source": [
    "![Matrix dot-product box diagram](./matrix_dot-product_box_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d2a00",
   "metadata": {},
   "source": [
    "more generally, the dot product between higher-dimensional tensors, following the same rules for shape compatibility as outlined earlier for the 2D case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, b, c, d) . (d,) -> (a, b , c)\n",
    "(a, b, c, d) . (d, e) -> (a, b, c, d, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c01835",
   "metadata": {},
   "source": [
    "##### Tensor Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45108a9d",
   "metadata": {},
   "source": [
    "tensor reshape is used to preprocessed the digits data before feeding it into the network, i.e.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab76e83",
   "metadata": {},
   "source": [
    "reshaping a tensor means rearranging its rows and columns to match a target shape, naturally, the reshaped tensor has the same total number of coefficients as the initial tensor. i.e.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08258c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0., 1.],\n",
    "            [2., 3.],\n",
    "            [4., 5.]])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888705e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [5.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((6, 1))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9016db47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.],\n",
       "       [3., 4., 5.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((2, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc664d",
   "metadata": {},
   "source": [
    "a special case of reshaping is 'transposition', transposing matrix means exchanging its rows and its columns, so that x[i,:] becomes x[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce64444b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 300)\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((300, 20))\n",
    "x = np.transpose(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42034410",
   "metadata": {},
   "source": [
    "##### Geometric Interpretation of Tensor Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b5108",
   "metadata": {},
   "source": [
    "the contents of the tensors manipulated by tensor operations can be interpreted as cordinates of points in some geometric space, all tensor operations have a geometirc interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27305a23",
   "metadata": {},
   "source": [
    "in general, elementary geometric operations such as affine transformations, rotations, and scaling, and so on can be expressed as tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd4203",
   "metadata": {},
   "source": [
    "for instance, a rotation of a 2D vector by an angle theta can be achieved via a dot product with a 2x2 matrix R = [u, v], where u and v are both vectors of the plane: u = [cos(theta), sin(theta)] and v = [-sin(theta), cos(theta)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3e5b3",
   "metadata": {},
   "source": [
    "##### A Geometric Interpretation of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea4110f",
   "metadata": {},
   "source": [
    "neural networks consist entirely of chains of tensor operations and that all of these tensor operations are just geometric transformations of the input data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038bf2e6",
   "metadata": {},
   "source": [
    "it follows that you can interpret a neural network as a very complex geometric transformation in a high-dimensional space, implemented via a long series of simple steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85391096",
   "metadata": {},
   "source": [
    "In 3D, the following mental image may prove useful. Imagine two sheets of colored paper: one red and one blue. Put one on top of the other. Now crumple them together into a small ball. That crumpled paper ball is your input data, and each sheet of paper is a class of data in a classification problem. What a neural network (or any other machine-learning model) is meant to do is figure out a transformation of the paper ball that would uncrumple it, so as to make the two classes cleanly separable again. With deep learning, this would be implemented as a series of simple transformations of the 3D space, such as those you could apply on the paper ball with your fingers, one movement at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d76164b",
   "metadata": {},
   "source": [
    "![Uncrumpling A Complicated Manifold of Data](./uncrumpling_a_complicated_manifold_of_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da3e521",
   "metadata": {},
   "source": [
    "Uncrumpling paper balls is what machine learning is about: finding neat representations for complex, highly folded data manifolds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e49b5",
   "metadata": {},
   "source": [
    "Deep Learning takes the approach of incrementally decomposing a complicated geometric transformation into a long chain of elementary ones, which is pretty much the strategy a human would follow to uncrumple a paper ball. Each layer in a deep network applies a transformation that disentagles the data a little, and a deep stack of layers makes tractable an extremly complicated disentanglement process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
