{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8afc8638",
   "metadata": {},
   "source": [
    "##### Scalars (0D tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7066dcd",
   "metadata": {},
   "source": [
    "A tensor that contains only one number is called a 'scalar' (or scalar tensor, or 0-dimensional tensor, or 0D tensor). In Numpy, a 'float32' or 'float64' number is a scalar tensor (or scalar array). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f6cab",
   "metadata": {},
   "source": [
    "We can display the number of axes of a Numpy tensor via 'ndim' attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0b077b",
   "metadata": {},
   "source": [
    "The number of axes of a tensor is also called its rank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26fd4e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49341961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e9760",
   "metadata": {},
   "source": [
    "##### Vectors (1D tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f2c21e",
   "metadata": {},
   "source": [
    "An array of numbers is caleld a vector, or 1D tensor. A 1D tensor is said to have exactly one axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "405a3651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  3,  6, 14])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([12, 3, 6, 14])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fafcf7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ee3f64",
   "metadata": {},
   "source": [
    "##### Matrices (2D tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd6043f",
   "metadata": {},
   "source": [
    "An array of vectors is a matrix, or 2D tensor. It can be visually interpret as a rectangular grid of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69bdc223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[5, 78, 2, 34, 0], \n",
    "             [6, 79, 3, 35, 1],\n",
    "             [7, 80, 4, 36, 2]])\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180559b",
   "metadata": {},
   "source": [
    "##### 3D tensors and higher-dimensional tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892e032",
   "metadata": {},
   "source": [
    "A 3D tensor packs such matrices in a new array, which can visually interpret as a cube of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45910a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[[5, 78, 2, 34, 0],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]], \n",
    "              [[5, 78, 2, 34, 0], \n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]],\n",
    "              [[5, 78, 2, 34, 0],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]]])\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a15f1",
   "metadata": {},
   "source": [
    "By packing 3D tensors in an array, we can create a 4D tensor, and so on. In deep learning, we generally manipulate tensors taht are 0D to 4D, although there can be 5D for processing video data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218062e6",
   "metadata": {},
   "source": [
    "##### Key attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc8caa",
   "metadata": {},
   "source": [
    "A tensor is defined by three key attributes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36bfec",
   "metadata": {},
   "source": [
    "'Number of axes (rank)' -- this is alos called the tensor's 'ndim' in Python libraries such as Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae59b0b",
   "metadata": {},
   "source": [
    "'Shape' -- this is a tuple of integers taht describes how many dimensions the tensor has along each axies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a5ab89",
   "metadata": {},
   "source": [
    "'Data type (usually called 'dtype' in Python libraries)' -- this is the type of the data contained in the tensor; for instance, a tensor's type could be 'float32', 'uint8', 'float64', and so on. On rare occasions, there may be a 'char' tensor. Note that string tensorrs don't exist in Numpy (or in most other libraries), because tensors live in preallocated, contiguous memory segments: and strings, being variable length, would preclude the use of this implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e7cd58",
   "metadata": {},
   "source": [
    "to make this more concrete, for the MNIST exmple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7240191",
   "metadata": {},
   "source": [
    "load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0d5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1328ee8",
   "metadata": {},
   "source": [
    "display the number of axes of the tensor 'train_images', the ndim attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b87ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(train_images.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1926a",
   "metadata": {},
   "source": [
    "here's its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3222491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea40a7",
   "metadata": {},
   "source": [
    "and this is its data type, the 'dtype' attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa929343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n"
     ]
    }
   ],
   "source": [
    "print(train_images.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0e85ab",
   "metadata": {},
   "source": [
    "So here is a 3D tensor of 8-bit integers. More precisely, it's an arry of 60,000 matrices of 28 x 8 integers. Each such matrix is a grayscale image, with coefficients between 0 and 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f1228",
   "metadata": {},
   "source": [
    "display the fourth digit (using the library Matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cedd26b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = train_images[4]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digit, cmap = plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf810e",
   "metadata": {},
   "source": [
    "##### Manipulating tensors in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494099d6",
   "metadata": {},
   "source": [
    "Previously, we selected a specific digit alongside the first axis using the syntax 'train_images[i]'. Selecting specific elements in a tensor is called tensor slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ffd1d8",
   "metadata": {},
   "source": [
    "the following selects digits #10 to #100 (#100 isn't included) and put them in an array of shape (90, 28, 28):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e39b16ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "my_slice = train_images[10:100]\n",
    "print(my_slice.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38e49f",
   "metadata": {},
   "source": [
    "it's equivalen to following more detailed notation, which specifies a start index and stop index for the slice along each tensor axis. Note that ':' is equivalen to selecting the entire axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f15851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_slice = train_images[10:100, :, :]\n",
    "my_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28b5da6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_slice = train_images[10:100, 0:28, 0:28]\n",
    "my_slice.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1b1be",
   "metadata": {},
   "source": [
    "For select between any two indices along each tensor axis, for instance, select 14 x 14 pixels in the bottom-right corner of all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "880e73dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_slice = train_images[:, 14:, 14:]\n",
    "plt.imshow(my_slice[4], cmap = plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d797c4",
   "metadata": {},
   "source": [
    "it's also possible to use negative indices. Much like negative indices in python lists, they indicate a position relative to the end of the current axis. In order to crop the images to patches of 14 x 14 pixels centered in the middle: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4e1483e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAANuElEQVR4nO3df4xddZnH8fdDaxGqDZRWI22zA4GwS4i7mEZQiRBRWrEw/rGQEiDtSth/2LWKBCHQmCVAlmiMNisaggIBAiSIQIjaNqiUJWtD+RG2pShdFKiMzFCDFiw/Jjz7x1w2w9CCe77nnrn2+34lk7m/nnmemcwn595zz7nfyEwk7f32me4BJHXDsEuVMOxSJQy7VAnDLlViZpfN5s2bl0NDQ1223Cu8/PLLjWt37NhR1Pull14qqt+1a1dRfYmFCxc2rp01a1ZR7507dzauPeiggxrXjoyM8OKLL8bu7us07ENDQ2zatKnLlnuFjRs3Nq698cYbi3pv2LChqH7z5s1F9SXOP//8xrUHH3xwUe/777+/ce3ZZ5/duHblypV7vM+n8VIlDLtUCcMuVaIo7BGxNCJ+FRHbIuKitoaS1L7GYY+IGcB3gM8CRwJnRMSRbQ0mqV0lW/aPAtsy86nMfA24FRhuZyxJbSsJ+wLg2UnXt/due4uI+OeI2BQRm8bGxgraSSpREvbdvXH/tvNlM/OazFycmYvnz59f0E5SiZKwbwcWTbq+EHiubBxJ/VIS9geBwyPikIiYBSwH7m5nLElta3y4bGaOR8S/AGuBGcAPMnNLa5NJalXRsfGZ+WPgxy3NIqmPPIJOqoRhlyrR6SmutbrtttuK6letWtW4tvTYhtJPHz7hhBMa177wwgtFvS+44IKi+hIlf7eS3/v555/f431u2aVKGHapEoZdqoRhlyph2KVKGHapEoZdqoRhlyph2KVKGHapEoZdqoRhlyph2KVKGHapEtWc4jo+Pl5U/+CDDzauPffcc4t6lyzZfPzxxxf1Xr16dVH9cccd17j21VdfLep9+umnN65du3ZtUe8Sixcvblz7wAMP7PE+t+xSJQy7VAnDLlXCsEuVKFnFdVFE/DwitkbEloho/kFpkvquZG/8OPCVzHw4It4PPBQR6zPz8ZZmk9Sixlv2zBzJzId7l3cCW9nNKq6SBkMrr9kjYgg4Gti4m/tcslkaAMVhj4j3AT8EvpSZf5p6v0s2S4OhKOwR8R4mgn5zZt7RzkiS+qFkb3wA3we2ZuY32xtJUj+UbNk/AZwNfCoiHu19ndzSXJJaVrI++38C0eIskvrII+ikShh2qRLVnM9+0003FdWfc845LU3y/3fSSSc1ri1dLnrOnDlF9SVKZ5/Oc9IXLVrUuHbFihWNa9/p/9wtu1QJwy5VwrBLlTDsUiUMu1QJwy5VwrBLlTDsUiUMu1QJwy5VwrBLlTDsUiUMu1QJwy5V4q/qFNdLL720ce2VV15Z1HviI/eaOe+884p6X3755Y1rp/MU1VJXXHHFdI/Q2Jo1axrXlnwK88yZe460W3apEoZdqoRhlyph2KVKtLH804yIeCQi7mljIEn90caWfRUTK7hKGmCla70tBD4HXNvOOJL6pXTL/i3gQuCNPT3AJZulwVCysOMyYDQzH3qnx7lkszQYShd2PDUifgvcysQCj2UrMUjqm8Zhz8yLM3NhZg4By4GfZeZZrU0mqVW+zy5VopUTYTLzF8Av2vhZkvrDLbtUCcMuVaLT89lHRka47LLLGteXnJO+7777Nq4FWLJkSePaq666qqj3fvvtV1Rf4pVXXimqX7duXePap59+uqh3ZjauXb16dVHv4eHhovp+cMsuVcKwS5Uw7FIlDLtUCcMuVcKwS5Uw7FIlDLtUCcMuVcKwS5Uw7FIlDLtUCcMuVcKwS5Xo9BTX0dFRrr766sb1Jcsml5yiCnDnnXcW1U+Xbdu2FdWfeeaZRfWbNm0qqi9x2mmnNa698MILW5xkMLhllyph2KVKGHapEoZdqkTpwo4HRMTtEfFERGyNiI+1NZikdpXujf828NPM/MeImAXs38JMkvqgcdgjYg7wSWAlQGa+BrzWzliS2lbyNP5QYAy4LiIeiYhrI2L21AdNXrL5jTf2uLKzpD4rCftM4CPAdzPzaOBl4KKpD5q8ZPM++7g/UJouJenbDmzPzI2967czEX5JA6hkyebfA89GxBG9m04EHm9lKkmtK90b/6/Azb098U8B/1Q+kqR+KAp7Zj4KLG5pFkl95B4zqRKGXapEp+ezj4+PMzY21mXL/7NmzZqi+tHR0ca11113XVHvu+66q3Htli1binrv3LmzqL7kMwhK36o966yzGtfOnv22Q0b+6rlllyph2KVKGHapEoZdqoRhlyph2KVKGHapEoZdqoRhlyph2KVKGHapEoZdqoRhlyph2KVKGHapEp2ezz5z5kzmzZvXuL7knPKhoaHGtVB2XvZ0WrBgQVH9nDlziuqfe+65xrUl/ysAp5xySlH93sYtu1QJwy5VwrBLlShdsvnLEbElIjZHxC0R8d62BpPUrsZhj4gFwBeBxZl5FDADWN7WYJLaVfo0fiawX0TMZGJt9ua7XiX1Vclab78DvgE8A4wAf8zMdVMf55LN0mAoeRp/IDAMHAIcDMyOiLd9ULdLNkuDoSR9nwZ+k5ljmfk6cAfw8XbGktS2krA/AxwbEfvHxOFlJwJb2xlLUttKXrNvBG4HHgb+u/ezrmlpLkktK12y+WvA11qaRVIfucdMqoRhlyrR6Smuhx12GNdff33j+mXLljWu3bFjR+NamJi9qeHh4aLeK1eubFw7d+7cot7Ll5cdFFlyimtpb72VW3apEoZdqoRhlyph2KVKGHapEoZdqoRhlyph2KVKGHapEoZdqoRhlyph2KVKGHapEoZdqoRhlyrR6fnss2fP5phjjmlcPzY21uI0ddiwYUNR/X333VdUX7LU9aGHHlrUW2/lll2qhGGXKmHYpUq8a9gj4gcRMRoRmyfdNjci1kfEk73vB/Z3TEml/pIt+/XA0im3XQTcm5mHA/f2rksaYO8a9szcAPxhys3DwA29yzcAn295Lkkta/qa/YOZOQLQ+/6BPT1w8pLNvnUmTZ++76CbvGTz/Pnz+91O0h40DfvzEfEhgN730fZGktQPTcN+N7Cid3kFcFc740jql7/krbdbgP8CjoiI7RFxDvDvwGci4kngM73rkgbYux4bn5ln7OGuE1ueRVIfeQSdVAnDLlWi01Nc1b1du3YV1Zecolpa75LN7XLLLlXCsEuVMOxSJQy7VAnDLlXCsEuVMOxSJQy7VAnDLlXCsEuVMOxSJQy7VAnDLlXCsEuVMOxSJTyffS+3ZMmS6R5BA8Itu1QJwy5VwrBLlWi6ZPPXI+KJiHgsIn4UEQf0d0xJpZou2bweOCozPwz8Gri45bkktazRks2ZuS4zx3tXfwks7MNsklrUxmv2LwA/aeHnSOqjorBHxCXAOHDzOzzG9dmlAdA47BGxAlgGnJmZuafHuT67NBgaHUEXEUuBrwLHZ+af2x1JUj80XbL5P4D3A+sj4tGI+F6f55RUqOmSzd/vwyyS+sgj6KRKGHapEp7iupdbu3btdI+gAeGWXaqEYZcqYdilShh2qRKGXaqEYZcqYdilShh2qRKGXaqEYZcqYdilShh2qRKGXaqEYZcqYdilSsQ7fDBs+80ixoCn3+Eh84AXOhrH3vbeG3v/TWbu9mOcOw37u4mITZm52N72tnf7fBovVcKwS5UYtLBfY29727s/Buo1u6T+GbQtu6Q+MexSJQYi7BGxNCJ+FRHbIuKiDvsuioifR8TWiNgSEau66j1phhkR8UhE3NNx3wMi4vaIeKL3+3+sw95f7v29N0fELRHx3j73+0FEjEbE5km3zY2I9RHxZO/7gR32/nrv7/5YRPwoIg7oR++ppj3sETED+A7wWeBI4IyIOLKj9uPAVzLz74BjgfM67P2mVcDWjnsCfBv4aWb+LfD3Xc0QEQuALwKLM/MoYAawvM9trweWTrntIuDezDwcuLd3vave64GjMvPDwK+Bi/vU+y2mPezAR4FtmflUZr4G3AoMd9E4M0cy8+He5Z1M/MMv6KI3QEQsBD4HXNtVz17fOcAn6S3QmZmvZeaLHY4wE9gvImYC+wPP9bNZZm4A/jDl5mHght7lG4DPd9U7M9dl5njv6i+Bhf3oPdUghH0B8Oyk69vpMHBviogh4GhgY4dtvwVcCLzRYU+AQ4Ex4LreS4hrI2J2F40z83fAN4BngBHgj5m5roveU3wwM0d6M40AH5iGGQC+APyki0aDEPbYzW2dvh8YEe8Dfgh8KTP/1FHPZcBoZj7URb8pZgIfAb6bmUcDL9O/p7Fv0XttPAwcAhwMzI6Is7roPWgi4hImXkre3EW/QQj7dmDRpOsL6fPTuski4j1MBP3mzLyjq77AJ4BTI+K3TLx0+VRE3NRR7+3A9sx881nM7UyEvwufBn6TmWOZ+TpwB/DxjnpP9nxEfAig9320y+YRsQJYBpyZHR3sMghhfxA4PCIOiYhZTOysubuLxhERTLxu3ZqZ3+yi55sy8+LMXJiZQ0z8zj/LzE62cJn5e+DZiDiid9OJwONd9Gbi6fuxEbF/7+9/ItOzg/JuYEXv8grgrq4aR8RS4KvAqZn55676kpnT/gWczMReyf8BLumw73FMvGR4DHi093XyNPz+JwD3dNzzH4BNvd/9TuDADnv/G/AEsBm4Edi3z/1uYWL/wOtMPKs5BziIib3wT/a+z+2w9zYm9lO9+T/3vS7+7h4uK1ViEJ7GS+qAYZcqYdilShh2qRKGXaqEYZcqYdilSvwvIg36w+8LTXkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_slice = train_images[:, 7:-7, 7:-7]\n",
    "plt.imshow(my_slice[4], cmap = plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1613650a",
   "metadata": {},
   "source": [
    "##### The notion of data batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0728cc5b",
   "metadata": {},
   "source": [
    "In general, the first axis (axis 0, because indexing starts at 0) in all data tensor in deep learning will be the samples axis (sometimes called the samples dimension). In ths MNIST example, samples are images of digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c652b10c",
   "metadata": {},
   "source": [
    "In addition, deep-learning models don't process an entire dataset at once; rather, they break the data into small batches. Concretely, here's one batch of our MNIST digits, with batch size of 128:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_images[;128]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29db76f",
   "metadata": {},
   "source": [
    "and here's the next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_images[128:256]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e051499",
   "metadata": {},
   "source": [
    "and the 'n'th batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb68fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_images[128 * n:128 * (n + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa1c6bf",
   "metadata": {},
   "source": [
    "when considering such a batch tensor, the first axis (axis 0) is called the batch axis or batch dimension. This is a term frequently encounter when using Keras and other deep-learning library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4540d",
   "metadata": {},
   "source": [
    "##### Real-world examples of data tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd08f87",
   "metadata": {},
   "source": [
    "To be more concretely, the data manipulated will almost always fall into on of the following categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a58a1cd",
   "metadata": {},
   "source": [
    "- 'Vector data' -- 2D tensors of shape (samples, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f2831f",
   "metadata": {},
   "source": [
    "- 'Timeseries data or sequence data' -- 3D tensors of shape (samples, timesteps, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b222f",
   "metadata": {},
   "source": [
    "- 'Images' -- 4D tensors of shape (samples, height, width, channels) or (samples, channels, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3e8d1",
   "metadata": {},
   "source": [
    "- 'Video' -- 5D tensors of shape (samples, frames, height, width, channels) or (samples, frames, channels, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb47d821",
   "metadata": {},
   "source": [
    "##### Vector data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb66e11b",
   "metadata": {},
   "source": [
    "This is  the most common case. In such a dataset, each single data point can be encoded as a vector, and thus a batch of data will be encoded as a 2D tensor (that is, an array of vectors), where the first axis is the samples axis and the second axis is the features axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb9b2b",
   "metadata": {},
   "source": [
    "- An actuarial dataset of people, where we consider each person’s age, ZIP code, and income. Each person can be characterized as a vector of 3 values, and thus an entire dataset of 100,000 people can be stored in a 2D tensor of shape (100000, 3) ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213caf9f",
   "metadata": {},
   "source": [
    "- A dataset of text documents, where we represent each document by the counts of how many times each word appears in it (out of a dictionary of 20,000 common words). Each document can be encoded as a vector of 20,000 values (onecount per word in the dictionary), and thus an entire dataset of 500 documents can be stored in a tensor of shape (500, 20000) ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82c1fb2",
   "metadata": {},
   "source": [
    "##### Timeseries data or sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c435b41",
   "metadata": {},
   "source": [
    "Whenever time matters in the data (or the notion of sequence order), it makes sense to store it in a 3D tensor wit an explicit time axis. Each sample can be encoded as a sequence of vectors (a 2D tensor), and thus a batch of data will be encoded as a 3D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1a51e",
   "metadata": {},
   "source": [
    "The time axis is always the second axis (axis of index 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6d6e4",
   "metadata": {},
   "source": [
    "- A dataset of stock prices. Every minute, we store the current price of the stock, the highest price in the past minute, and the lowest price in the past minute. Thus every minute is encoded as a 3D vector, an entire day of trading is encoded as a 2D tensor of shape (390, 3) (there are 390 minutes in a trading day), and 250 days’ worth of data can be stored in a 3D tensor of shape (250, 390, 3) . Here, each sample would be one day’s worth of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ccd02c",
   "metadata": {},
   "source": [
    "- A dataset of tweets, where we encode each tweet as a sequence of 280 characters out of an alphabet of 128 unique characters. In this setting, each character can be encoded as a binary vector of size 128 (an all-zeros vector except for a 1 entry at the index corresponding to the character). Then each tweet can be encoded as a 2D tensor of shape (280, 128) , and a dataset of 1 million tweets can be stored in a tensor of shape (1000000, 280, 128)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39360058",
   "metadata": {},
   "source": [
    "##### Image data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc4db2",
   "metadata": {},
   "source": [
    "Images typically have three dimensions: height, width, and color depth. Although grayscale images (like our MNIST digits) have only a single color channel and could thus be stored in 2D tensors, by convention image tensors are always 3D , with a one-dimensional color channel for grayscale images. A batch of 128 grayscale images of size 256 × 256 could thus be stored in a tensor of shape (128, 256, 256, 1) , and a batch of 128 color images could be stored in a tensor of shape (128, 256, 256, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e7a44",
   "metadata": {},
   "source": [
    "There are two conventions for shapes of images tensors: the channels-last convention (used by TensorFlow) and the channels-first convention (used by Theano). The TensorFlow machine-learning framework, from Google, places the color-depth axis at the end: (samples, height, width, color_depth) . Meanwhile, Theano places the color depth axis right after the batch axis: (samples, color_depth, height, width). With the Theano convention, the previous examples would become (128, 1, 256, 256) and (128, 3, 256, 256) . The Keras framework provides support for both formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc32fe",
   "metadata": {},
   "source": [
    "##### Video data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a33d8d",
   "metadata": {},
   "source": [
    "Video data is one of the few types of real-world data for which you’ll need 5D tensors. A video can be understood as a sequence of frames, each frame being a color image. Because each frame can be stored in a 3D tensor (height, width, color_depth) , a sequence of frames can be stored in a 4D tensor (frames, height, width, color_depth) , and thus a batch of different videos can be stored in a 5D tensor of shape (samples, frames, height, width, color_depth) ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd232de",
   "metadata": {},
   "source": [
    "For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames per second would have 240 frames. A batch of four such video clips would be stored in a tensor of shape (4, 240, 144, 256, 3) . That’s a total of 106,168,320 values! If the dtype of the tensor was float32 , then each value would be stored in 32 bits, so the tensor would represent 405 MB. Heavy! Videos you encounter in real life are much lighter, because they aren’t stored in float32 , and they’re typically compressed by a large factor (such as in the MPEG format)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
