{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd75b64f",
   "metadata": {},
   "source": [
    "this session reaches the end of the chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb7049",
   "metadata": {},
   "source": [
    "the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25550836",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((60000, 28 * 28))\n",
    "test_images = test_images.astype('float32') /255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca815d6",
   "metadata": {},
   "source": [
    "the input images are stored in numpy tensors, which are here formatted as 'float32' tensors of shape (60000, 784) (training data) and (10000, 784) (test data), respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46154daf",
   "metadata": {},
   "source": [
    "the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbd59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab3c540",
   "metadata": {},
   "source": [
    "this network consists of a chain of two Dense layers, that each layer applies a few simple tensor operations to the input data, and that these operations involve weight tensors, which are attributes of the layers, are where the knowledge of the network persists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c7cb8b",
   "metadata": {},
   "source": [
    "the network-compilation step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop', \n",
    "                loss='categorical_crossentropy',\n",
    "                metrices=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e44b28",
   "metadata": {},
   "source": [
    "the 'categorical_crossentropy' is the loss function that's used as a feedback signal for learning the weight tensors, and which the training phase will attempt to minimise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b8793b",
   "metadata": {},
   "source": [
    "and also this reduction of the loss happens via mini-batch stochastic gradient descent are defined by the 'rmsprop' optimizer passed as the first argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02a16ed",
   "metadata": {},
   "source": [
    "the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b25130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8768c807",
   "metadata": {},
   "source": [
    "the network will then iterate on the training data in mini-batches of 128 samples, 5 times over (each iteration over all the training data is called an epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7d9723",
   "metadata": {},
   "source": [
    "at each iteration, the network will compute the gradietns of the weights with regard to the loss on the batch, and update the weights accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f53a3d3",
   "metadata": {},
   "source": [
    "after these 5 epochs, the network will have performed 2345 ftadietn updates (469 per epoch), and the loss of the network will be sufficiently low that the network will be capable of classifying handwritten digits with high accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a204352",
   "metadata": {},
   "source": [
    "##### Chapter Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556db5a1",
   "metadata": {},
   "source": [
    "- learning means finding a comination fo model parameters taht minimises a loss function for a given set of training data samples and their corresponding targets\n",
    "- learning happens by drawing random batches of data samples and their targets, and computing the gradient of the network parameters with respect to teh loss on the batch, the network parameters are then moved a bit (the magnitude of the move is defined by the learning rate) in opposite direction from the gradient\n",
    "- the entire learning process is made possible by the fact that neural networks are chains of differentiable tensor operations, and thus it's possible to apply the chain rule of derivation to find the gradient function mapping the current parameters and current batch of data to a gradient value\n",
    "- two key concepts you'll see frequently in future chapters are loss and optimisers, these are the two things you need to define before you begin deeding data into a network\n",
    "- the loss is the quantity you'll attempt to minimise during training, so it should represent a measure of success for the task you're trying to solve\n",
    "- the optimiser specifies the exact way in which the gradient of the loss will be used to update parameters: for instance, it could be the RMSProp optimiser, SGD with momentum, and so on "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
